# ğŸ—ï¸ Mise Ã  Jour Architecture - Novembre 2024

## ğŸ“Š Changements Majeurs

### 1. âŒ SupprimÃ© : Zookeeper
**Raison** : Kafka 3.3+ utilise KRaft (Kafka Raft Metadata Mode)  
**Avantage** : 
- Architecture simplifiÃ©e (-1 conteneur)
- Moins de ressources (~500 MB RAM Ã©conomisÃ©e)
- Plus performant
- Futur de Kafka (Zookeeper dÃ©prÃ©ciÃ© en 2024)

### 2. âœ… AjoutÃ© : MinIO (Data Lake)
**Remplace** : Google Cloud Storage (GCS)  
**Raison** : DÃ©veloppement 100% local  
**Avantages** :
- API S3-compatible (standard de l'industrie)
- Stockage illimitÃ© (limitÃ© par disque local)
- Console web intÃ©grÃ©e
- 0â‚¬ de coÃ»t
- Compatible avec Spark (via S3A connector)

**Configuration** :
- Endpoint: http://localhost:9000 (API)
- Console: http://localhost:9001
- Credentials: minioadmin / minioadmin123

**Buckets crÃ©Ã©s automatiquement** :
- `datalake` : DonnÃ©es gÃ©nÃ©rales
- `raw-data` : DonnÃ©es brutes non traitÃ©es
- `processed-data` : DonnÃ©es traitÃ©es par Spark
- `scraped-jobs` : Offres d'emploi scrapÃ©es (HTML/JSON)
- `scraped-cvs` : CVs scrapÃ©s (PDF/DOCX/JSON)
- `backups` : Sauvegardes

### 3. âœ… AjoutÃ© : Apache Superset (BI)
**Remplace** : Google Looker Studio  
**Raison** : Solution BI 100% open-source et locale  
**Avantages** :
- Dashboards interactifs puissants
- SQL Lab intÃ©grÃ© (IDE SQL)
- Nombreux connecteurs (BigQuery, PostgreSQL, etc.)
- Customisable et extensible
- Gratuit et open-source

**Configuration** :
- URL: http://localhost:8088
- Login: admin / admin
- Base de donnÃ©es : PostgreSQL (partagÃ©e avec Airflow)

### 4. âœ… AjoutÃ© : Couche de Scraping
**Nouveau composant** : Container dÃ©diÃ© au web scraping  
**Technologies** :
- Scrapy : Framework de scraping
- Selenium : Browser automation
- Playwright : Alternative moderne Ã  Selenium
- BeautifulSoup : Parsing HTML
- spaCy : NLP pour extraction
- pdfplumber : Parsing de CVs PDF

**Topics Kafka crÃ©Ã©s** :
- `job-offers-raw` : Offres brutes HTML
- `job-offers-parsed` : Offres parsÃ©es JSON
- `cvs-raw` : CVs bruts (PDF/DOCX)
- `cvs-parsed` : CVs parsÃ©s JSON
- `scraping-errors` : Erreurs de scraping
- `scraper-commands` : Commandes pour les scrapers
- `scraper-status` : Statut des scrapers

### 5. âœ… AjoutÃ© : Redis
**Raison** : Cache pour Airflow et Superset  
**Avantages** :
- AmÃ©liore les performances
- RÃ©duit la charge sur PostgreSQL
- Standard pour Superset

### 6. ğŸ”„ ModifiÃ© : PostgreSQL
**Changement** : Supporte maintenant 2 bases de donnÃ©es  
- `airflow` : MÃ©tadonnÃ©es Airflow
- `superset` : MÃ©tadonnÃ©es Superset

Script d'init : `docker/postgres/init-multiple-databases.sh`

## ğŸ”— Nouvelle Architecture de DonnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   WEB SCRAPING                          â”‚
â”‚ (Educarriere, LinkedIn, Goafricaonline, Macarrierepro)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                KAFKA (KRaft Mode)                       â”‚
â”‚  Topics: job-offers-raw, cvs-raw, scraping-errors       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                           â”‚
         â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MINIO (S3)    â”‚         â”‚  SPARK CLUSTER   â”‚
â”‚   Data Lake     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Processing      â”‚
â”‚   â€¢ Raw HTML    â”‚         â”‚  â€¢ Parsing       â”‚
â”‚   â€¢ Raw PDF     â”‚         â”‚  â€¢ NLP           â”‚
â”‚   â€¢ Parquet     â”‚         â”‚  â€¢ Matching      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                           â”‚
         â”‚                           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    AIRFLOW              â”‚
         â”‚    Orchestration        â”‚
         â”‚    â€¢ Scraping DAGs      â”‚
         â”‚    â€¢ Processing DAGs    â”‚
         â”‚    â€¢ Loading DAGs       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    BIGQUERY             â”‚
         â”‚    Data Warehouse       â”‚
         â”‚    â€¢ fact_jobs          â”‚
         â”‚    â€¢ fact_cvs           â”‚
         â”‚    â€¢ dim_skills         â”‚
         â”‚    â€¢ agg_matching       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    SUPERSET             â”‚
         â”‚    BI Dashboards        â”‚
         â”‚    â€¢ Job Market Analysisâ”‚
         â”‚    â€¢ Skills Trends      â”‚
         â”‚    â€¢ Salary Analysis    â”‚
         â”‚    â€¢ Candidate Matching â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“Š Diagramme Mermaid

```mermaid
flowchart TD
    A[WEB SCRAPING<br/>Educarriere, LinkedIn<br/>Goafricaonline, Macarrierepro] --> B[KAFKA KRaft Mode<br/>Topics: job-offers-raw<br/>cvs-raw, scraping-errors]
    
    B --> C[MINIO S3<br/>Data Lake<br/>â€¢ Raw HTML<br/>â€¢ Raw PDF<br/>â€¢ Parquet]
    B --> D[SPARK CLUSTER<br/>Processing<br/>â€¢ Parsing<br/>â€¢ NLP<br/>â€¢ Matching]
    
    D --> C
    C --> E[AIRFLOW<br/>Orchestration<br/>â€¢ Scraping DAGs<br/>â€¢ Processing DAGs<br/>â€¢ Loading DAGs]
    D --> E
    
    E --> F[BIGQUERY<br/>Data Warehouse<br/>â€¢ fact_jobs<br/>â€¢ fact_cvs<br/>â€¢ dim_skills<br/>â€¢ agg_matching]
    
    F --> G[SUPERSET<br/>BI Dashboards<br/>â€¢ Job Market Analysis<br/>â€¢ Skills Trends<br/>â€¢ Salary Analysis<br/>â€¢ Candidate Matching]
    
    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#e8f5e9
    style D fill:#f3e5f5
    style E fill:#fff3e0
    style F fill:#e0f2f1
    style G fill:#fce4ec
```

## ğŸ“¦ Services Docker

### Services Principaux (11 conteneurs)

1. **kafka** : Broker Kafka (KRaft mode)
2. **schema-registry** : Validation schÃ©mas Avro
3. **kafka-ui** : Interface web Kafka
4. **minio** : Data Lake S3-compatible
5. **minio-init** : Initialisation buckets (one-shot)
6. **spark-master** : Spark cluster manager
7. **spark-worker-1** : Worker node 1
8. **spark-worker-2** : Worker node 2
9. **postgres** : BDD (Airflow + Superset)
10. **redis** : Cache (Airflow + Superset)
11. **airflow-webserver** : Interface Airflow
12. **airflow-scheduler** : Ordonnanceur Airflow
13. **airflow-init** : Initialisation Airflow (one-shot)
14. **superset** : Interface Superset
15. **superset-init** : Initialisation Superset (one-shot)
16. **scrapers** : Container de scraping
17. **jupyter** : Notebooks PySpark

**Total** : 17 conteneurs (dont 2 one-shot)  
**Actifs en permanence** : 15 conteneurs

## ğŸ”Œ Ports UtilisÃ©s

| Service | Port | Usage |
|---------|------|-------|
| Kafka | 9092 | Kafka broker (externe) |
| Kafka | 29092 | Kafka broker (interne) |
| Schema Registry | 8081 | API REST schÃ©mas |
| Kafka UI | 8080 | Interface web |
| MinIO API | 9000 | API S3 |
| MinIO Console | 9001 | Interface web |
| Spark Master | 7077 | Spark cluster |
| Spark Master UI | 8082 | Interface web |
| Spark Worker 1 | 8083 | Interface web |
| Spark Worker 2 | 8084 | Interface web |
| PostgreSQL | 5432 | Base de donnÃ©es |
| Redis | 6379 | Cache |
| Airflow | 8085 | Interface web |
| Superset | 8088 | Interface web |
| Jupyter | 8888 | JupyterLab |
| Spark Job UI | 4040 | Spark application UI |

## ğŸ’¾ Volumes Persistants

```yaml
volumes:
  kafka_data:           # Messages Kafka
  minio_data:           # Fichiers MinIO (Data Lake)
  spark_master_data:    # DonnÃ©es Spark Master
  spark_worker_1_data:  # DonnÃ©es Spark Worker 1
  spark_worker_2_data:  # DonnÃ©es Spark Worker 2
  postgres_data:        # Bases de donnÃ©es PostgreSQL
  redis_data:           # Cache Redis
  superset_home:        # Configuration Superset
```

## ğŸ”§ Configuration Spark + MinIO

Spark est configurÃ© pour accÃ©der Ã  MinIO via le protocole S3A :

```python
spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .getOrCreate()

# Lire depuis MinIO
df = spark.read.parquet("s3a://datalake/data/jobs.parquet")

# Ã‰crire dans MinIO
df.write.mode("overwrite").parquet("s3a://processed-data/output/")
```

## ğŸš€ Workflow Complet

### 1. Scraping
```
Scraper Daemon (Python)
  â†’ Scrape sites web (Indeed, LinkedIn, etc.)
  â†’ Produit messages Kafka (job-offers-raw, cvs-raw)
  â†’ Sauvegarde HTML/PDF dans MinIO (scraped-jobs, scraped-cvs)
```

### 2. Processing (Spark Streaming)
```
Spark Streaming
  â†’ Consomme topics Kafka (job-offers-raw, cvs-raw)
  â†’ Parse HTML/PDF
  â†’ Extraction NLP (compÃ©tences, salaires, localisations)
  â†’ DÃ©duplication
  â†’ Sauvegarde Parquet dans MinIO (processed-data)
  â†’ Produit topics Kafka (job-offers-parsed, cvs-parsed)
```

### 3. Loading (Airflow + Spark Batch)
```
Airflow DAG (quotidien)
  â†’ Lance Spark Batch Job
  â†’ Lit donnÃ©es MinIO (processed-data)
  â†’ AgrÃ©gations et transformations
  â†’ Chargement dans BigQuery
  â†’ Notification fin de traitement
```

### 4. Analytics (Superset)
```
Superset
  â†’ Se connecte Ã  BigQuery
  â†’ Dashboards interactifs
  â†’ RequÃªtes SQL Lab
  â†’ Exports et rapports
```

## ğŸ“Š SchÃ©ma BigQuery

### Tables Dimension (dim_*)

```sql
-- dim_skills : CompÃ©tences
CREATE TABLE dim_skills (
  skill_id STRING,
  skill_name STRING,
  skill_category STRING,
  created_at TIMESTAMP
);

-- dim_companies : Entreprises
CREATE TABLE dim_companies (
  company_id STRING,
  company_name STRING,
  industry STRING,
  location STRING
);

-- dim_locations : Localisations
CREATE TABLE dim_locations (
  location_id STRING,
  city STRING,
  region STRING,
  country STRING
);
```

### Tables de Fait (fact_*)

```sql
-- fact_job_offers : Offres d'emploi
CREATE TABLE fact_job_offers (
  job_id STRING,
  title STRING,
  company_id STRING,
  location_id STRING,
  salary_min FLOAT64,
  salary_max FLOAT64,
  contract_type STRING,
  remote_option BOOLEAN,
  skills ARRAY<STRING>,
  posted_date DATE,
  scraped_at TIMESTAMP
)
PARTITION BY posted_date
CLUSTER BY company_id, location_id;

-- fact_candidates : Candidats/CVs
CREATE TABLE fact_candidates (
  candidate_id STRING,
  skills ARRAY<STRING>,
  years_experience INT64,
  education_level STRING,
  desired_salary FLOAT64,
  location_id STRING,
  scraped_at TIMESTAMP
)
PARTITION BY DATE(scraped_at);
```

### Tables AgrÃ©gÃ©es (agg_*)

```sql
-- agg_matching_scores : Scores de matching
CREATE TABLE agg_matching_scores (
  job_id STRING,
  candidate_id STRING,
  match_score FLOAT64,
  skill_match_pct FLOAT64,
  salary_match_pct FLOAT64,
  location_match_pct FLOAT64,
  calculated_at TIMESTAMP
)
PARTITION BY DATE(calculated_at)
CLUSTER BY job_id, candidate_id;
```

## ğŸ¯ Prochaines Ã‰tapes

1. **ImplÃ©menter les scrapers** (Indeed, LinkedIn, WTTJ, Apec)
2. **CrÃ©er les jobs Spark** (parsing, NLP, matching)
3. **CrÃ©er les DAGs Airflow** (orchestration)
4. **CrÃ©er les dashboards Superset** (visualisation)
5. **Configurer BigQuery** (schÃ©ma et connexion)

## ğŸ“š Ressources Utiles

- [Kafka KRaft Documentation](https://kafka.apache.org/documentation/#kraft)
- [MinIO Documentation](https://min.io/docs/minio/linux/index.html)
- [Apache Superset Documentation](https://superset.apache.org/docs/intro)
- [Scrapy Documentation](https://docs.scrapy.org/)
- [Spark S3A Documentation](https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html)

---

**Architecture mise Ã  jour le : 24 novembre 2024**

