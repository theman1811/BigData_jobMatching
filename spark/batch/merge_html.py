#!/usr/bin/env python3
"""
Fusionne les petits fichiers HTML bruts en blocs plus gros pour r√©duire
le nombre de fichiers/splits lors du parsing.

Source: s3a://<SOURCE_BUCKET>/*.html (par d√©faut scraped-jobs)
Cible : s3a://<SOURCE_BUCKET>/<TARGET_PREFIX>/part-*.html (par d√©faut prefix "merged")

Param√®tres d'environnement :
- SOURCE_BUCKET (def: scraped-jobs)
- TARGET_PREFIX (def: merged)
- MERGE_PARTITIONS (def: 50)  -> nombre de fichiers de sortie
- SPARK_MASTER (def: spark://spark-master:7077)
"""

import os
import sys
from pyspark.sql import SparkSession


def create_spark_session():
    spark_master = os.getenv("SPARK_MASTER", "spark://spark-master:7077")
    return SparkSession.builder \
        .appName("MergeSmallHtmlFiles") \
        .master(spark_master) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.files.maxPartitionBytes", os.getenv("MAX_PARTITION_BYTES", "134217728")) \
        .getOrCreate()


def main():
    source_bucket = os.getenv("SOURCE_BUCKET", "scraped-jobs")
    target_prefix = os.getenv("TARGET_PREFIX", "merged").strip("/")
    merge_partitions = int(os.getenv("MERGE_PARTITIONS", "50"))

    source_path = f"s3a://{source_bucket}/*.html"
    target_path = f"s3a://{source_bucket}/{target_prefix}"

    print("üöÄ Fusion des fichiers HTML")
    print(f"   Source : {source_path}")
    print(f"   Cible  : {target_path}")
    print(f"   Fichiers de sortie : ~{merge_partitions}")

    spark = None
    try:
        spark = create_spark_session()
        sc = spark.sparkContext

        # Supprimer le r√©pertoire cible s'il existe (√©vite FileAlreadyExistsException)
        hadoop_conf = sc._jsc.hadoopConfiguration()
        fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(
            sc._jvm.java.net.URI(target_path), hadoop_conf
        )
        target_uri = sc._jvm.java.net.URI(target_path)
        target_path_obj = sc._jvm.org.apache.hadoop.fs.Path(target_uri.getPath())
        
        if fs.exists(target_path_obj):
            print(f"‚ö†Ô∏è  R√©pertoire {target_path} existe d√©j√†, suppression...")
            fs.delete(target_path_obj, True)  # True = r√©cursif
            print(f"‚úÖ R√©pertoire {target_path} supprim√©")

        # wholeTextFiles lit chaque fichier entier ; minPartitions permet de regrouper
        rdd = sc.wholeTextFiles(source_path, minPartitions=merge_partitions).map(lambda kv: kv[1])

        # Coalesce pour forcer le nombre de fichiers de sortie
        rdd.coalesce(merge_partitions).saveAsTextFile(target_path)

        print("‚úÖ Fusion termin√©e")
    except Exception as e:
        print(f"‚ùå Erreur fusion: {e}")
        sys.exit(1)
    finally:
        if spark:
            spark.stop()
            print("‚úÖ Session Spark arr√™t√©e")


if __name__ == "__main__":
    main()
