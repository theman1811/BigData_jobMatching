# ğŸš€ BigData Job Matching - Architecture Big Data Scalable

## ğŸ“‹ Description

Plateforme Big Data scalable pour l'ingestion, la centralisation et l'analyse de donnÃ©es provenant des offres d'emploi et CVs de candidats sur internet.

**Mode : Hybride** (DÃ©veloppement local + GCP BigQuery)

## ğŸ—ï¸ Architecture ModernisÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COUCHE DE SCRAPING (Web Scraping)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Scrapers] â†’ Indeed, LinkedIn, WTTJ, Apec             â”‚
â”‚  â€¢ Scrapy/Selenium pour extraction                      â”‚
â”‚  â€¢ Anti-ban & Rate limiting                             â”‚
â”‚  â€¢ Rotation User-Agent                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DÃ‰VELOPPEMENT LOCAL (Docker Compose)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Kafka KRaft] â†’ [Spark] â†’ [Airflow] â†’ [Jupyter]       â”‚
â”‚   Ingestion      Process   Orchestration  Development   â”‚
â”‚                                                          â”‚
â”‚  [MinIO S3] â† Data Lake (stockage local)                â”‚
â”‚  [Superset] â† BI Dashboards (local)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLOUD GCP (Free Tier)                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [BigQuery] â†’ Data Warehouse + Analytics                â”‚
â”‚  â€¢ 10 GB stockage gratuit                               â”‚
â”‚  â€¢ 1 TB queries/mois gratuit                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Stack Technique

### Local (Docker)
- **Apache Kafka (KRaft)** : Ingestion temps rÃ©el (sans Zookeeper!)
- **Schema Registry** : Gestion des schÃ©mas Kafka
- **MinIO** : Data Lake S3-compatible (local, illimitÃ©)
- **Apache Spark** : Traitement distribuÃ© (PySpark)
- **Apache Airflow** : Orchestration des pipelines
- **Apache Superset** : BI & Dashboards (open-source)
- **Jupyter Notebook** : DÃ©veloppement et expÃ©rimentation
- **PostgreSQL** : Base de donnÃ©es (Airflow + Superset)
- **Redis** : Cache (Airflow + Superset)

### Scraping
- **Scrapy** : Framework de scraping
- **Selenium/Playwright** : Browser automation (sites JavaScript)
- **BeautifulSoup** : Parsing HTML
- **spaCy** : NLP pour extraction d'informations
- **pdfplumber** : Parsing de CVs PDF

### Cloud (GCP Free Tier)
- **BigQuery** : Data Warehouse (10 GB + 1 TB queries/mois gratuit)

## ğŸ“¦ PrÃ©requis

- Docker Desktop (ou Docker Engine + Docker Compose)
- Python 3.11+
- Compte Google Cloud Platform (uniquement pour BigQuery)
- 10 GB RAM minimum (12 GB recommandÃ©)
- 20 GB espace disque

## ğŸš€ DÃ©marrage Rapide

### 1. Cloner et configurer

```bash
# Copier le fichier de configuration
cp .env.example .env

# Ã‰diter .env et configurer vos paramÃ¨tres (notamment GCP si besoin)
nano .env
```

### 2. DÃ©marrer tous les services

```bash
# Rendre les scripts exÃ©cutables
chmod +x start.sh stop.sh status.sh clean.sh

# DÃ©marrer la plateforme
./start.sh
```

â³ **Attendre 3-4 minutes** que tous les services dÃ©marrent.

### 3. VÃ©rifier le statut

```bash
./status.sh
```

## ğŸŒ AccÃ©der aux Interfaces Web

| Service | URL | Credentials |
|---------|-----|-------------|
| **Kafka UI** | http://localhost:8080 | Aucun |
| **MinIO Console** | http://localhost:9001 | user: `minioadmin`<br>pass: `minioadmin123` |
| **Spark Master** | http://localhost:8082 | Aucun |
| **Spark Worker 1** | http://localhost:8083 | Aucun |
| **Spark Worker 2** | http://localhost:8084 | Aucun |
| **Airflow** | http://localhost:8085 | user: `airflow`<br>pass: `airflow` |
| **Superset** | http://localhost:8088 | user: `admin`<br>pass: `admin` |
| **Jupyter** | http://localhost:8888 | token: `bigdata2024` |

## ğŸ“Š Phase 6 - Superset (BigQuery)

0) Se connecter Ã  GCP via Docker (service account) :
```bash
docker run --platform=linux/amd64 --rm \
  -v "$PWD":/work \
  -v "$PWD/credentials/gcp-service-account.json":/sa.json \
  -w /work google/cloud-sdk:alpine \
  sh -c 'gcloud auth activate-service-account --key-file=/sa.json --project=<PROJECT_ID> && echo "GCP auth OK"'
```

1) CrÃ©er les vues BigQuery dÃ©diÃ©es aux dashboards :
```bash
bq query --use_legacy_sql=false < bigquery/queries/superset_views.sql
```

2) Ajouter la connexion BigQuery dans Superset (UI)  
`bigquery://<project_id>/?credentials_path=/opt/airflow/credentials/bq-service-account.json`

3) Publier ces datasets dans Superset :
- `jobmatching_dw.v_offres_daily` (date, source, secteur, localisation, contrat, salaires)
- `jobmatching_dw.v_top_competences` (competences, secteur, localisation, source, date)
- `jobmatching_dw.v_salaires_secteur_ville` (moyennes + p50 par secteur/ville)
- `jobmatching_dw.v_geo_offres` (lat/long pour cartes)

4) Dashboards recommandÃ©s (ordre de livraison) :
- MarchÃ© de lâ€™Emploi : courbe offres/jour, top compÃ©tences, carte, salaires
- Tendances Salariales : Ã©volution, comparaisons villes, salaire vs expÃ©rience (si dispo)
- Analyse CompÃ©tences : Ã©mergentes, combinaisons, demande par secteur
- Matching (quand prÃªt) : scores, meilleures recommandations

5) Performance/UX :
- Cache Superset activÃ© (Redis) ; ajuster TTL si besoin
- Colonnes date marquÃ©es dans chaque dataset pour le time grain
- Filtres globaux conseillÃ©s : date, source, secteur, localisation

## ğŸ“ Structure du Projet

```
bigData_jobMatching/
â”‚
â”œâ”€â”€ docker-compose.yml         # Orchestration des services
â”‚
â”œâ”€â”€ docker/                    # Configurations Docker
â”‚   â”œâ”€â”€ postgres/             # Init scripts PostgreSQL
â”‚   â””â”€â”€ scrapers/             # Scraper container
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ scraper_daemon.py
â”‚
â”œâ”€â”€ config/                    # Fichiers de configuration
â”‚   â”œâ”€â”€ spark-defaults.conf   # Configuration Spark
â”‚   â””â”€â”€ superset_config.py    # Configuration Superset
â”‚
â”œâ”€â”€ data/                      # DonnÃ©es locales
â”‚   â”œâ”€â”€ raw/                  # DonnÃ©es brutes
â”‚   â”œâ”€â”€ processed/            # DonnÃ©es traitÃ©es
â”‚   â””â”€â”€ scraped/              # Pages scrapÃ©es
â”‚
â”œâ”€â”€ kafka/                     # Configuration Kafka
â”‚   â”œâ”€â”€ producers/            # Producteurs (scrapers)
â”‚   â”œâ”€â”€ consumers/            # Consommateurs
â”‚   â””â”€â”€ schemas/              # SchÃ©mas Avro
â”‚
â”œâ”€â”€ spark/                     # Jobs Spark
â”‚   â”œâ”€â”€ streaming/            # Spark Streaming
â”‚   â”œâ”€â”€ batch/                # Batch processing
â”‚   â””â”€â”€ nlp/                  # Traitement NLP
â”‚
â”œâ”€â”€ airflow/                   # DAGs Airflow
â”‚   â”œâ”€â”€ dags/                 # DÃ©finitions des DAGs
â”‚   â”œâ”€â”€ plugins/              # Plugins personnalisÃ©s
â”‚   â””â”€â”€ logs/                 # Logs
â”‚
â”œâ”€â”€ bigquery/                  # SQL BigQuery
â”‚   â”œâ”€â”€ schemas/              # SchÃ©mas de tables
â”‚   â””â”€â”€ queries/              # RequÃªtes SQL
â”‚
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â””â”€â”€ exploration/          # Notebooks d'exploration
â”‚
â”œâ”€â”€ scripts/                   # Scripts utilitaires
â”‚   â”œâ”€â”€ setup/                # Scripts d'installation
â”‚   â””â”€â”€ gcp/                  # Scripts GCP
â”‚
â””â”€â”€ docs/                      # Documentation
    â”œâ”€â”€ architecture.md       # Architecture dÃ©taillÃ©e
    â””â”€â”€ setup_gcp.md          # Guide configuration GCP
```

## ğŸ”„ Flux de DonnÃ©es

### Pipeline Scraping â†’ BigQuery

```
1. SCRAPING
   Scrapers â†’ Kafka (job-offers-raw, cvs-raw)
   
2. STOCKAGE RAW
   Kafka â†’ MinIO (HTML/PDF bruts)
   
3. TRAITEMENT
   Spark Streaming consomme Kafka
   â†’ Parsing HTML/PDF
   â†’ Extraction NLP (compÃ©tences, salaires, localisations)
   â†’ DÃ©duplication
   
4. STOCKAGE TRAITÃ‰
   Spark â†’ MinIO (Parquet structurÃ©)
   
5. DATA WAREHOUSE
   Airflow â†’ Chargement MinIO vers BigQuery
   
6. VISUALISATION
   Superset â†’ Connexion BigQuery
   â†’ Dashboards interactifs
```

## ğŸ¯ Cas d'Usage

### 1. Analyse du MarchÃ© de l'Emploi
- Tendances des offres d'emploi par secteur
- Salaires moyens par compÃ©tence et localisation
- CompÃ©tences les plus demandÃ©es
- Ã‰volution temporelle du marchÃ©

### 2. Matching Offres-CVs
- Score de compatibilitÃ© candidat-offre
- Recommandations personnalisÃ©es
- Gap analysis de compÃ©tences
- PrÃ©diction de salaire

### 3. Intelligence Ã‰conomique
- StratÃ©gies de recrutement des entreprises
- Ã‰mergence de nouvelles technologies
- Cartographie des bassins d'emploi
- PrÃ©visions du marchÃ©

## ğŸ“Š Avantages de la Nouvelle Architecture

### âœ… Kafka KRaft (vs Zookeeper)
- **-1 conteneur** : Architecture simplifiÃ©e
- **Plus rapide** : Meilleures performances
- **Plus moderne** : Standard depuis Kafka 3.3+
- **Moins de RAM** : ~500 MB Ã©conomisÃ©s

### âœ… MinIO (vs GCS)
- **100% local** : Pas de dÃ©pendance cloud pour le dev
- **IllimitÃ©** : LimitÃ© seulement par votre disque
- **API S3** : Compatible avec tous les outils AWS
- **0â‚¬** : Gratuit pour toujours

### âœ… Superset (vs Looker Studio)
- **Open-source** : ContrÃ´le total
- **Plus puissant** : Nombreuses fonctionnalitÃ©s BI
- **Local** : Pas besoin de compte Google
- **Extensible** : Plugins et customisation

## ğŸ’° CoÃ»t EstimÃ©

**DÃ©veloppement local** : 0 â‚¬  
**MinIO (Data Lake)** : 0 â‚¬  
**Superset (BI)** : 0 â‚¬  
**BigQuery (Free Tier)** : 0 â‚¬ (10 GB + 1 TB queries/mois)  
**Total** : 0 â‚¬ âœ…

Avec les crÃ©dits Ã©tudiants GCP (300$), vous avez une marge confortable !

## ğŸ”§ Configuration BigQuery

### 1. CrÃ©er un projet GCP

```bash
# Aller sur https://console.cloud.google.com
# CrÃ©er un nouveau projet
```

### 2. Activer BigQuery API

```bash
gcloud services enable bigquery.googleapis.com
```

### 3. CrÃ©er un Service Account

```bash
# CrÃ©er le service account
gcloud iam service-accounts create bigdata-sa \
  --display-name="BigData Service Account"

# Donner les permissions BigQuery
gcloud projects add-iam-policy-binding YOUR_PROJECT_ID \
  --member="serviceAccount:bigdata-sa@YOUR_PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/bigquery.dataEditor"

# CrÃ©er la clÃ© JSON
gcloud iam service-accounts keys create ./config/gcp-service-account.json \
  --iam-account=bigdata-sa@YOUR_PROJECT_ID.iam.gserviceaccount.com
```

### 4. Configurer le .env

```bash
# Ã‰diter .env
GCP_PROJECT_ID=your-project-id
GCP_DATASET_ID=job_matching_dw
```

## ğŸ§ª Tests Rapides

### Tester Kafka

```bash
# CrÃ©er un topic
docker exec -it bigdata_kafka kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --topic test-topic

# Lister les topics
docker exec -it bigdata_kafka kafka-topics --list \
  --bootstrap-server localhost:9092
```

### Tester MinIO

```bash
# Ouvrir http://localhost:9001
# Login: minioadmin / minioadmin123
# VÃ©rifier que les buckets sont crÃ©Ã©s
```

### Tester Spark + MinIO

```python
# Dans Jupyter (http://localhost:8888)
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("TestMinIO") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .getOrCreate()

# CrÃ©er un DataFrame
df = spark.createDataFrame([(1, "test")], ["id", "value"])

# Ã‰crire dans MinIO
df.write.parquet("s3a://datalake/test/data.parquet")

print("âœ… Test rÃ©ussi!")
```

## ğŸ›‘ ArrÃªter la Plateforme

```bash
./stop.sh
```

## ğŸ§¹ Nettoyer ComplÃ¨tement

âš ï¸ **Attention** : Supprime toutes les donnÃ©es !

```bash
./clean.sh
```

## ğŸ“š Documentation ComplÃ¨te

- **docs/architecture.md** : Architecture technique dÃ©taillÃ©e
- **docs/setup_gcp.md** : Guide complet GCP
- **COMMANDS.md** : Commandes utiles
- **QUICKSTART.md** : Guide de dÃ©marrage rapide

## ğŸ†˜ ProblÃ¨mes Courants

### Port dÃ©jÃ  utilisÃ©

```bash
# Trouver le processus
lsof -i :8080

# Modifier le port dans docker-compose.yml
```

### Kafka ne dÃ©marre pas

```bash
# VÃ©rifier les logs
docker logs bigdata_kafka

# Supprimer les volumes et redÃ©marrer
./clean.sh
./start.sh
```

### MinIO inaccessible

```bash
# VÃ©rifier le conteneur
docker logs bigdata_minio

# VÃ©rifier que le port 9001 est libre
lsof -i :9001
```

## ğŸ“ˆ Roadmap

### Phase 1 : Infrastructure âœ…
- [x] Docker Compose
- [x] Kafka KRaft
- [x] MinIO
- [x] Spark
- [x] Airflow
- [x] Superset

### Phase 2 : Scraping (en cours)
- [ ] Scraper Indeed
- [ ] Scraper LinkedIn
- [ ] Scraper Welcome to the Jungle
- [ ] Scraper Apec
- [ ] Anti-ban logic

### Phase 3 : Traitement
- [ ] Jobs Spark Streaming
- [ ] Parsing HTML â†’ JSON
- [ ] Extraction NLP
- [ ] DÃ©duplication
- [ ] Chargement BigQuery

### Phase 4 : BI
- [ ] Dashboards Superset
- [ ] Analyse marchÃ© emploi
- [ ] Matching offres-CVs
- [ ] PrÃ©dictions ML

## ğŸ‘¥ Ã‰quipe

Projet acadÃ©mique - Big Data & BI  
Technologies : Kafka, Spark, Airflow, MinIO, Superset, BigQuery

## ğŸ“„ Licence

Projet acadÃ©mique - Usage Ã©ducatif uniquement

---

**Stack 100% Open-Source | DÃ©veloppement 100% Local | Cloud Hybride** ğŸš€
