# ğŸ“‹ Phase 5 : DAGs Airflow - Orchestration des Pipelines

## ğŸ¯ Objectif
CrÃ©er l'orchestration complÃ¨te des pipelines Big Data avec Apache Airflow.

## ğŸ“ Fichiers CrÃ©Ã©s

### DAGs Principaux
- **`bigquery_load_dag.py`** : Chargement des donnÃ©es vers BigQuery
- **`scraping_daily_dag.py`** : Scraping quotidien automatisÃ©
- **`processing_spark_dag.py`** : Processing Spark des donnÃ©es

### Outils
- **`test_connections.py`** : Test des connexions Airflow

## ğŸ”„ Flux de DonnÃ©es OrchestrÃ©

```
Scraping (2h) â†’ Processing (4h) â†’ BigQuery Loading
     â†“              â†“              â†“
   Kafka        MinIO â†’ Spark    BigQuery
   Raw          Processed       Warehouse
```

## ğŸ• Programmation des DAGs

| DAG | Schedule | Description |
|-----|----------|-------------|
| `scraping_daily` | `0 2 * * *` | 2h du matin - Scraping quotidien |
| `processing_spark` | `0 4 * * *` | 4h du matin - Processing Spark |
| `bigquery_load` | Manuel/Daily | Chargement BigQuery (manuel ou quotidien) |

## ğŸš€ Utilisation

### 1. DÃ©marrer les Services
```bash
./start.sh
```

### 2. AccÃ©der Ã  Airflow
- **URL** : http://localhost:8085
- **Login** : `airflow` / `airflow`

### 3. Tester les Connexions
```bash
cd airflow
python test_connections.py
```

### 4. Activer les DAGs
1. Aller dans **Admin â†’ Variables** et dÃ©finir :
   - `PROJECT_ROOT` : `/opt/airflow/project`
   - `GCP_PROJECT_ID` : `bigdata-jobmatching-test`
   - `BIGQUERY_DATASET` : `jobmatching_dw`

2. Aller dans **Admin â†’ Connections** et vÃ©rifier/crÃ©er :
   - `spark_default` : Connexion Spark Master
   - `bigquery_default` : Connexion BigQuery
   - `minio_default` : Connexion MinIO (optionnel)

3. Dans **DAGs**, activer les DAGs :
   - `scraping_daily`
   - `processing_spark`
   - `bigquery_load`

### 5. Tester les DAGs
1. **Trigger manuel** : Bouton "Trigger DAG" pour chaque DAG
2. **Monitorer** : Onglet "Graph View" et "Tree View"
3. **Logs** : Cliquer sur une tÃ¢che â†’ "View Logs"

## âš™ï¸ Configuration Requise

### Variables d'Environnement
```bash
# Dans Airflow Admin â†’ Variables
PROJECT_ROOT=/opt/airflow/project
GCP_PROJECT_ID=bigdata-jobmatching-test
BIGQUERY_DATASET=jobmatching_dw
MINIO_BUCKET=processed-data
```

### Credentials GCP
Placer le fichier service account dans le conteneur Airflow :
```bash
# Copier vers le conteneur
docker cp credentials/bq-service-account.json bigdata_airflow_webserver:/opt/airflow/credentials/
```

### Connexions Airflow

#### Spark Connection (`spark_default`)
- **Conn Type** : `spark`
- **Host** : `spark://spark-master`
- **Port** : `7077`

#### BigQuery Connection (`bigquery_default`)
- **Conn Type** : `google_cloud_platform`
- **Project ID** : `bigdata-jobmatching-test`
- **Keyfile Path** : `/opt/airflow/credentials/bq-service-account.json`

#### MinIO Connection (`minio_default`)
- **Conn Type** : `s3`
- **Host** : `minio`
- **Port** : `9000`
- **Login** : `minioadmin`
- **Password** : `minioadmin123`
- **Schema** : `http`

## ğŸ”§ DÃ©pannage

### DAGs non visibles
```bash
# RedÃ©marrer Airflow
docker restart bigdata_airflow_scheduler bigdata_airflow_webserver
```

### Connexions Ã©chouent
1. VÃ©rifier que tous les services sont dÃ©marrÃ©s : `./status.sh`
2. Tester les connexions : `python airflow/test_connections.py`
3. VÃ©rifier les logs des tÃ¢ches dans l'interface Airflow

### Jobs Spark Ã©chouent
1. VÃ©rifier la connexion Spark : `docker logs bigdata_spark_master`
2. VÃ©rifier que les fichiers sont accessibles dans MinIO
3. Tester manuellement : `./scripts/spark/run_parse_jobs.sh`

## ğŸ“Š Monitoring

### MÃ©triques Ã  Surveiller
- **Temps d'exÃ©cution** : Chaque tÃ¢che < 2h
- **Taux de succÃ¨s** : > 95%
- **Volume de donnÃ©es** : Nombre d'offres traitÃ©es
- **Erreurs** : Logs d'erreurs dÃ©taillÃ©s

### Alertes (TODO)
- Ã‰chec de scraping
- Ã‰chec de processing
- Volume anormal de donnÃ©es
- Erreurs BigQuery

## ğŸ¯ Ã‰tat Actuel

### âœ… ImplÃ©mentÃ©
- [x] Structure des 3 DAGs principaux
- [x] Orchestration sÃ©quentielle des jobs Spark
- [x] Scraping parallÃ¨le des 4 sources
- [x] Chargement BigQuery automatisÃ©
- [x] Script de test des connexions

### ğŸ”„ Ã€ ComplÃ©ter
- [ ] ImplÃ©mentation `consume_cvs.py` (Phase 4)
- [ ] Tests end-to-end des DAGs
- [ ] Notifications par email/Slack
- [ ] Monitoring avancÃ© des mÃ©triques
- [ ] Gestion des erreurs amÃ©liorÃ©e

## ğŸš€ Prochaines Ã‰tapes

1. **CrÃ©er `consume_cvs.py`** (Phase 4 manquante)
2. **Tester les DAGs** en production
3. **ImplÃ©menter les notifications**
4. **Ajouter le monitoring** dÃ©taillÃ©
5. **Phase 6** : Dashboards Superset

---
**Phase 5 : ~80% complÃ©tÃ©e** ğŸ¯

*DAGs opÃ©rationnels - Pipeline orchestrÃ© - PrÃªt pour production*
