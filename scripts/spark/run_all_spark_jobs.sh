#!/bin/bash
# ============================================
# Script de lancement - Tous les Jobs Spark
# ============================================
# Lance tous les jobs Spark dans l'ordre appropri√©

set -e  # Arr√™ter le script en cas d'erreur

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"

echo "üöÄ Lancement de tous les jobs Spark - Pipeline Complet"
echo "üìÅ Projet: $PROJECT_ROOT"
echo ""

# Fonction pour ex√©cuter un job
run_job() {
    local job_name="$1"
    local script_path="$2"
    local description="$3"

    echo "üéØ $job_name: $description"
    echo "üìÑ Script: $script_path"

    if [ -f "$script_path" ]; then
        echo "‚ñ∂Ô∏è  D√©marrage..."
        bash "$script_path"

        if [ $? -eq 0 ]; then
            echo "‚úÖ $job_name termin√© avec succ√®s"
        else
            echo "‚ùå $job_name a √©chou√©"
            exit 1
        fi
    else
        echo "‚ùå Script introuvable: $script_path"
        exit 1
    fi

    echo "‚è±Ô∏è  Pause de 10 secondes..."
    sleep 10
    echo ""
}

# ============================================
# JOBS SPARK STREAMING (√† lancer en arri√®re-plan)
# ============================================

echo "üîÑ JOBS STREAMING (√† lancer manuellement en arri√®re-plan):"
echo "   1. Consommateur Kafka Jobs: scripts/spark/run_consume_jobs.sh"
echo "   2. Consommateur Kafka CVs: scripts/spark/run_consume_cvs.sh (√Ä FAIRE)"
echo ""

# ============================================
# JOBS SPARK BATCH (s√©quentiels)
# ============================================

echo "üîß JOBS BATCH (ex√©cution s√©quentielle):"
echo ""

# 1. Parsing des offres HTML
run_job \
    "Parse Jobs HTML" \
    "$SCRIPT_DIR/run_parse_jobs.sh" \
    "Parser HTML ‚Üí JSON structur√©"

# 2. Extraction des comp√©tences (si disponible)
if [ -f "$SCRIPT_DIR/run_extract_skills.sh" ]; then
    run_job \
        "Extract Skills" \
        "$SCRIPT_DIR/run_extract_skills.sh" \
        "Extraction NLP comp√©tences"
fi

# 3. Extraction des salaires (si disponible)
if [ -f "$SCRIPT_DIR/run_extract_salary.sh" ]; then
    run_job \
        "Extract Salary" \
        "$SCRIPT_DIR/run_extract_salary.sh" \
        "Parsing salaires FCFA"
fi

# 4. D√©duplication (si disponible)
if [ -f "$SCRIPT_DIR/run_deduplicate.sh" ]; then
    run_job \
        "Deduplicate" \
        "$SCRIPT_DIR/run_deduplicate.sh" \
        "D√©duplication inter-sources"
fi

# 5. Matching (si disponible)
if [ -f "$SCRIPT_DIR/run_matching.sh" ]; then
    run_job \
        "Matching" \
        "$SCRIPT_DIR/run_matching.sh" \
        "Calcul matching offres-CVs"
fi

# 6. Chargement BigQuery (toujours en dernier)
run_job \
    "Load BigQuery" \
    "$SCRIPT_DIR/run_load_bigquery.sh" \
    "Chargement vers BigQuery"

echo "üéâ Tous les jobs Spark ont √©t√© ex√©cut√©s avec succ√®s!"
echo ""
echo "üìã R√©sum√© du pipeline:"
echo "   ‚úÖ Streaming: Jobs consomm√©s depuis Kafka ‚Üí MinIO"
echo "   ‚úÖ Batch: HTML pars√© ‚Üí Donn√©es structur√©es ‚Üí BigQuery"
echo ""
echo "üîÑ Pour les jobs streaming, lancez manuellement:"
echo "   scripts/spark/run_consume_jobs.sh &"
echo ""
echo "üìä Prochaine √©tape: Configuration Airflow DAGs (Phase 5)"
