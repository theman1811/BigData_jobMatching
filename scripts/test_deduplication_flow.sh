#!/bin/bash
# Script de test de la d√©duplication BigQuery
# Ce script d√©clenche le DAG et surveille les logs pour v√©rifier la d√©duplication

set -e

PROJECT_DIR="/Users/nedio/Documents/programmation/school/BigData_jobMatching"
cd "$PROJECT_DIR"

echo "=============================================="
echo "üß™ TEST DE D√âDUPLICATION BIGQUERY"
echo "=============================================="
echo ""

# V√©rifier que les services sont actifs
echo "üîç V√©rification des services..."
if ! docker ps | grep -q "bigdata_airflow_scheduler"; then
    echo "‚ùå Airflow n'est pas d√©marr√©. Lancez ./start.sh"
    exit 1
fi
echo "‚úÖ Services actifs"
echo ""

# Compter les fichiers dans MinIO
echo "üì¶ √âtat de MinIO:"
docker exec bigdata_scrapers python3 -c "
from minio import Minio
client = Minio('minio:9000', access_key='minioadmin', secret_key='minioadmin123', secure=False)
scraped = len(list(client.list_objects('scraped-jobs', recursive=True)))
print(f'   - scraped-jobs: {scraped} fichiers HTML')
try:
    parsed = len([o for o in client.list_objects('processed-data', prefix='jobs_parsed/', recursive=True) if o.object_name.endswith('.parquet')])
    print(f'   - jobs_parsed: {parsed} fichiers parquet')
except:
    print(f'   - jobs_parsed: 0 fichiers parquet')
"
echo ""

# D√©clencher le DAG
echo "üöÄ D√©clenchement du DAG processing_spark..."
TRIGGER_OUTPUT=$(docker exec bigdata_airflow_scheduler airflow dags trigger processing_spark 2>&1)
echo "$TRIGGER_OUTPUT"

# Extraire le dag_run_id
DAG_RUN_ID=$(echo "$TRIGGER_OUTPUT" | grep -oE 'manual__[0-9T:+-]+' | head -1)

if [ -z "$DAG_RUN_ID" ]; then
    echo "‚ö†Ô∏è  Impossible de d√©terminer le dag_run_id. Continuons quand m√™me..."
    DAG_RUN_ID="latest"
fi

echo "üìù DAG Run ID: $DAG_RUN_ID"
echo ""

# Attendre un peu que le DAG d√©marre
echo "‚è≥ Attente du d√©marrage du DAG (30 secondes)..."
sleep 30

# Fonction pour r√©cup√©rer les logs d'une t√¢che
get_task_logs() {
    local task_id=$1
    echo ""
    echo "üìã Logs de la t√¢che: $task_id"
    echo "----------------------------------------"
    
    # Essayer de r√©cup√©rer les logs
    docker exec bigdata_airflow_scheduler airflow tasks logs processing_spark "$task_id" "$DAG_RUN_ID" 2>/dev/null || {
        echo "‚ö†Ô∏è  Logs pas encore disponibles pour $task_id"
        return 1
    }
}

# Attendre que la t√¢che spark_parse_jobs soit termin√©e
echo "‚è≥ Attente de la fin de spark_parse_jobs..."
MAX_WAIT=300  # 5 minutes max
ELAPSED=0
INTERVAL=15

while [ $ELAPSED -lt $MAX_WAIT ]; do
    STATE=$(docker exec bigdata_airflow_scheduler airflow tasks state processing_spark spark_parse_jobs "$DAG_RUN_ID" 2>/dev/null || echo "pending")
    
    echo "   √âtat de spark_parse_jobs: $STATE (${ELAPSED}s/${MAX_WAIT}s)"
    
    if [ "$STATE" = "success" ]; then
        echo "‚úÖ spark_parse_jobs termin√© avec succ√®s"
        break
    elif [ "$STATE" = "failed" ]; then
        echo "‚ùå spark_parse_jobs a √©chou√©"
        get_task_logs "spark_parse_jobs"
        exit 1
    elif [ "$STATE" = "running" ]; then
        echo "   ‚è≥ T√¢che en cours d'ex√©cution..."
    fi
    
    sleep $INTERVAL
    ELAPSED=$((ELAPSED + INTERVAL))
done

if [ $ELAPSED -ge $MAX_WAIT ]; then
    echo "‚è∞ Timeout atteint. Le DAG prend trop de temps."
    echo "Vous pouvez v√©rifier manuellement sur http://localhost:8080"
    exit 1
fi

echo ""
echo "=============================================="
echo "üìä R√âSULTATS DU TEST"
echo "=============================================="
echo ""

# Afficher l'√©tat final du DAG
echo "üìà √âtat final du DAG:"
docker exec bigdata_airflow_scheduler airflow dags state processing_spark "$DAG_RUN_ID" 2>/dev/null || echo "√âtat inconnu"
echo ""

# Compter les fichiers pars√©s apr√®s ex√©cution
echo "üì¶ √âtat de MinIO apr√®s processing:"
docker exec bigdata_scrapers python3 -c "
from minio import Minio
client = Minio('minio:9000', access_key='minioadmin', secret_key='minioadmin123', secure=False)
try:
    parsed = len([o for o in client.list_objects('processed-data', prefix='jobs_parsed/', recursive=True) if o.object_name.endswith('.parquet')])
    print(f'   - jobs_parsed: {parsed} fichiers parquet')
except:
    print(f'   - jobs_parsed: 0 fichiers parquet')
"
echo ""

echo "=============================================="
echo "‚úÖ Test termin√© !"
echo "=============================================="
echo ""
echo "Pour voir les logs complets du DAG:"
echo "  docker exec bigdata_airflow_scheduler airflow dags show processing_spark"
echo ""
echo "Pour voir l'interface Airflow:"
echo "  http://localhost:8080"
echo ""
echo "Pour v√©rifier BigQuery manuellement, ex√©cutez dans le conteneur Spark:"
echo "  docker exec bigdata_spark_master /opt/spark/bin/spark-submit --master spark://spark-master:7077 /opt/airflow/spark/batch/load_to_bigquery.py"
echo ""
