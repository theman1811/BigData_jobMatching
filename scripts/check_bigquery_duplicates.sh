#!/bin/bash
# Script rapide pour vÃ©rifier l'Ã©tat de BigQuery et dÃ©tecter les doublons

echo "=============================================="
echo "ðŸ” VÃ‰RIFICATION BIGQUERY - Ã‰tat des doublons"
echo "=============================================="
echo ""

# Afficher le dernier run du DAG bigquery_load
echo "ðŸ“Š DerniÃ¨res exÃ©cutions du DAG bigquery_load:"
docker exec bigdata_airflow_scheduler airflow dags list-runs -d bigquery_load --no-backfill --output table 2>/dev/null | head -10
echo ""

# VÃ©rifier les logs du dernier run pour voir la dÃ©duplication
echo "ðŸ” Recherche des messages de dÃ©duplication dans les logs rÃ©cents:"
echo ""

# Chercher dans les logs Spark du dernier jour
docker exec bigdata_spark_master find /opt/spark/logs -name "*.out" -mtime -1 -exec grep -l "VÃ©rification des offres existantes" {} \; 2>/dev/null | head -1 | xargs -I {} sh -c 'echo "ðŸ“„ Fichier: {}" && grep -A 2 -E "VÃ©rification des offres|nouvelles offres|offres existantes|Aucune nouvelle" {}'

echo ""
echo "=============================================="
echo "ðŸ’¡ Commandes utiles"
echo "=============================================="
echo ""
echo "1. DÃ©clencher manuellement le DAG BigQuery:"
echo "   docker exec bigdata_airflow_scheduler airflow dags trigger bigquery_load"
echo ""
echo "2. Voir l'interface Airflow:"
echo "   http://localhost:8080"
echo ""
echo "3. Voir les logs Spark en temps rÃ©el:"
echo "   docker logs -f bigdata_spark_master"
echo ""
echo "4. Compter les fichiers dans MinIO:"
echo "   docker exec bigdata_scrapers python3 -c \\"
echo "     from minio import Minio; \\"
echo "     client = Minio('minio:9000', access_key='minioadmin', secret_key='minioadmin123', secure=False); \\"
echo "     print(f'Scraped: {len(list(client.list_objects(\\\"scraped-jobs\\\", recursive=True)))}'); \\"
echo "     print(f'Parsed: {len([o for o in client.list_objects(\\\"processed-data\\\", prefix=\\\"jobs_parsed/\\\", recursive=True) if o.object_name.endswith(\\\".parquet\\\")])}'); \\"
echo "   \\"
echo ""
