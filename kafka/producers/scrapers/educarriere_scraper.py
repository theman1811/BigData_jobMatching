#!/usr/bin/env python3
"""
==========================================
Educarriere Scraper - CÃ´te d'Ivoire
==========================================
Scraper pour https://emploi.educarriere.ci/nos-offres
809 offres d'emploi & de stage - Structure trÃ¨s claire
"""

import re
import time
from typing import List, Dict, Any
from datetime import datetime

import requests
from bs4 import BeautifulSoup

from .base_scraper import BaseJobScraperCI


class EducarriereScraper(BaseJobScraperCI):
    """Scraper pour Educarriere.ci - Plateforme d'emploi Ã©ducative ivoirienne"""

    BASE_URL = "https://emploi.educarriere.ci"
    OFFERS_URL = "https://emploi.educarriere.ci/nos-offres"

    # Types d'emploi mappÃ©s
    JOB_TYPES_MAPPING = {
        'Emploi (CDI)': 'CDI',
        'Emploi': 'CDI',  # DÃ©faut pour Emploi
        'Stage': 'Stage',
        'Freelance': 'Freelance',
        'CDD': 'CDD',
        'CDD CDI': 'CDD/CDI'
    }

    def scrape_page(self, page: int = 1) -> str:
        """Scrape une page d'offres d'emploi"""
        url = f"{self.OFFERS_URL}?page={page}" if page > 1 else self.OFFERS_URL

        self.logger.info(f"ğŸ“„ Scraping page {page}: {url}")

        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            # Rotation User-Agent entre les pages
            if page % 3 == 0:  # Tous les 3 pages
                self.rotate_user_agent()

            return response.text

        except requests.RequestException as e:
            self.logger.error(f"âŒ Erreur requÃªte page {page}: {e}")
            return ""

    def parse_job_offer(self, job_element) -> Dict[str, Any]:
        """Parse une offre d'emploi depuis l'Ã©lÃ©ment HTML"""
        try:
            # Analyser tout le texte de l'Ã©lÃ©ment
            all_text = job_element.get_text(separator=' | ', strip=True)
            self.logger.debug(f"ğŸ“ Parsing offre: {all_text[:200]}...")

            # Extraire le code
            code_match = re.search(r'Code:\s*(\d+)', all_text)
            code = code_match.group(1) if code_match else ""

            # Extraire les dates
            date_edition_match = re.search(r"Date d'Ã©dition:\s*([^|]+)", all_text)
            date_edition = date_edition_match.group(1).strip() if date_edition_match else ""

            date_limite_match = re.search(r"Date limite:\s*([^|]+)", all_text)
            date_limite = date_limite_match.group(1).strip() if date_limite_match else ""

            # Extraire le titre - chercher dans les Ã©lÃ©ments de titre ou liens
            title = "Titre non trouvÃ©"

            # Essayer les Ã©lÃ©ments de titre
            title_candidates = job_element.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'strong', 'b', 'a'])
            for candidate in title_candidates:
                candidate_text = candidate.get_text(strip=True)
                if (len(candidate_text) > 5 and
                    not candidate_text.startswith('Code:') and
                    not 'Date' in candidate_text and
                    not candidate_text.isdigit()):
                    title = candidate_text
                    break

            # Si toujours pas trouvÃ©, chercher dans le texte complet
            if title == "Titre non trouvÃ©":
                lines = [line.strip() for line in all_text.split(' | ') if line.strip()]
                for line in lines:
                    if (len(line) > 10 and
                        not line.startswith('Code:') and
                        not 'Date' in line and
                        not line.replace('/', '').replace('-', '').isdigit()):
                        title = line
                        break

            # Chercher l'URL
            offer_link = job_element.find('a', href=True)
            offer_url = ""
            if offer_link:
                offer_url = offer_link['href']
                if offer_url and not offer_url.startswith('http'):
                    offer_url = f"{self.BASE_URL}{offer_url}"

            # DÃ©terminer le type de contrat depuis le titre
            contract_type = 'CDI'
            title_lower = title.lower()
            if 'stage' in title_lower:
                contract_type = 'Stage'
            elif 'cdd' in title_lower:
                contract_type = 'CDD'
            elif 'freelance' in title_lower or 'indÃ©pendant' in title_lower:
                contract_type = 'Freelance'

            # CrÃ©er l'ID unique
            job_id = self.create_job_id('educarriere', code or title)

            # Extraire les compÃ©tences du titre
            detected_skills = super()._extract_skills_from_text(title)

            # Structure de donnÃ©es standardisÃ©e
            job_data = {
                'job_id': job_id,
                'title': title,
                'company': 'Entreprise confidentielle',  # Educarriere n'affiche pas toujours l'entreprise
                'location': 'CÃ´te d\'Ivoire',
                'description': title,
                'contract_type': contract_type,
                'job_type': contract_type,
                'source': 'educarriere_ci',
                'source_url': offer_url,
                'posted_date': date_edition,
                'application_deadline': date_limite,
                'scraped_at': datetime.now().isoformat(),
                'country': 'CÃ´te d\'Ivoire',

                # MÃ©tadonnÃ©es spÃ©cifiques Educarriere
                'educarriere_code': code,
                'educarriere_date_edition': date_edition,
                'educarriere_date_limite': date_limite,

                # DonnÃ©es enrichies
                'salary': None,
                'remote_option': False,
                'skills': detected_skills,
                'education_level': None,
                'experience_years': None,

                # CatÃ©gorisation
                'industry': self._guess_industry(title, 'Entreprise confidentielle'),
                'seniority_level': self._guess_seniority(title)
            }

            self.logger.debug(f"âœ… Offre parsÃ©e: {code} - {title}")
            return job_data

        except Exception as e:
            self.logger.error(f"âŒ Erreur parsing offre: {e}")
            return None

    def parse_jobs_from_html(self, html: str) -> List[Dict[str, Any]]:
        """Parse toutes les offres d'une page HTML"""
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        jobs = []

        # DEBUG: Analyser le contenu de la page
        page_title = soup.find('title')
        self.logger.info(f"ğŸ” DEBUG: Titre de la page: {page_title.text.strip() if page_title else 'AUCUN TITRE'}")

        body = soup.find('body')
        if body:
            self.logger.info(f"ğŸ” DEBUG: Body trouvÃ©, longueur: {len(str(body))} caractÃ¨res")
            # Chercher tous les liens
            all_links = body.find_all('a', href=True)
            self.logger.info(f"ğŸ” DEBUG: {len(all_links)} liens trouvÃ©s")
            for i, link in enumerate(all_links[:3]):
                self.logger.info(f"   {i+1}. {link.text.strip()[:30]}... -> {link['href'][:50]}")

            # Chercher tous les Ã©lÃ©ments de texte significatifs
            text_elements = body.find_all(string=lambda t: t and len(t.strip()) > 20)
            self.logger.info(f"ğŸ” DEBUG: {len(text_elements)} Ã©lÃ©ments texte (>20 chars)")
            for i, text in enumerate(text_elements[:3]):
                self.logger.info(f"   {i+1}. {text.strip()[:50]}...")
        else:
            self.logger.error("âŒ DEBUG: AUCUN body trouvÃ© dans la page!")
            self.logger.info(f"ğŸ” DEBUG: Contenu HTML brut (500 premiers chars): {html[:500]}")

        # DEBUG: Chercher tous les titres potentiels
        all_titles = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5'], string=lambda t: t and len(t.strip()) > 5)
        self.logger.info(f"ğŸ” DEBUG: {len(all_titles)} titres H1-H5 trouvÃ©s")
        for i, title in enumerate(all_titles[:5]):
            self.logger.info(f"   H{i+1}. {title.text.strip()[:50]}...")

        # DEBUG: Chercher tous les Ã©lÃ©ments avec "Code:"
        all_codes = soup.find_all(string=re.compile(r'Code:'))
        self.logger.info(f"ğŸ” DEBUG: {len(all_codes)} Ã©lÃ©ments avec 'Code:' trouvÃ©s")
        for i, code in enumerate(all_codes[:3]):
            self.logger.info(f"   {i+1}. {code.strip()}")

        # Chercher les offres - Nouvelle approche : chercher les conteneurs qui ont les patterns complets
        # Pattern: Ã©lÃ©ment qui contient "Code:" ET "Date d'Ã©dition:"
        job_elements = []

        # Chercher tous les Ã©lÃ©ments (div, article, etc.) qui contiennent les patterns d'une offre complÃ¨te
        all_containers = soup.find_all(['div', 'article', 'li', 'tr'])

        for container in all_containers:
            container_text = container.get_text(separator=' ', strip=True)

            # VÃ©rifier si cet Ã©lÃ©ment contient les patterns caractÃ©ristiques d'une offre d'emploi
            has_code = re.search(r'Code:\s*\d+', container_text)
            has_date_edition = re.search(r"Date d'Ã©dition:", container_text)
            has_date_limite = re.search(r"Date limite:", container_text)

            # Et qu'il a du texte qui ressemble Ã  un titre d'offre (longueur significative)
            text_length = len(container_text)
            has_title_like_text = text_length > 50 and text_length < 500  # Longueur rÃ©aliste pour une offre

            if has_code and (has_date_edition or has_date_limite) and has_title_like_text:
                # Filtrer les faux positifs (articles de news, etc.)
                if not any(skip_word in container_text.lower() for skip_word in [
                    'formation lbc/ft', 'agboville', 'coupure d\'eau', 'actualitÃ©', 'news'
                ]):
                    job_elements.append(container)
                    self.logger.debug(f"âœ… Offre trouvÃ©e: {container_text[:100]}...")

        self.logger.info(f"ğŸ” TrouvÃ© {len(job_elements)} Ã©lÃ©ments d'offres valides")

        self.logger.info(f"ğŸ” TrouvÃ© {len(job_elements)} Ã©lÃ©ments d'offres sur la page")

        for job_elem in job_elements:
            job_data = self.parse_job_offer(job_elem)
            if job_data:
                jobs.append(job_data)

        return jobs

    def get_total_pages(self, html: str) -> int:
        """DÃ©termine le nombre total de pages"""
        if not html:
            return 1

        soup = BeautifulSoup(html, 'html.parser')

        # Chercher "Page nÂ° 1 sur 29"
        page_text = soup.find(string=re.compile(r'Page\s+nÂ°\s+\d+\s+sur\s+\d+'))
        if page_text:
            match = re.search(r'Page\s+nÂ°\s+\d+\s+sur\s+(\d+)', page_text)
            if match:
                return int(match.group(1))

        # Chercher dans la pagination
        pagination = soup.find('div', class_=re.compile(r'pagination|pager'))
        if pagination:
            page_links = pagination.find_all(['a', 'span'], string=re.compile(r'\d+'))
            if page_links:
                # Prendre le numÃ©ro le plus Ã©levÃ©
                page_numbers = []
                for link in page_links:
                    try:
                        num = int(link.text.strip())
                        page_numbers.append(num)
                    except ValueError:
                        continue
                if page_numbers:
                    return max(page_numbers)

        # DÃ©faut: 29 pages comme vu dans les rÃ©sultats
        self.logger.warning("âš ï¸ Nombre de pages non trouvÃ©, utilisation de 29 par dÃ©faut")
        return 29

    def scrape_jobs(self, max_pages: int = None, delay_min: float = 2.0, delay_max: float = 5.0) -> List[Dict[str, Any]]:
        """Scrape toutes les offres d'emploi d'Educarriere"""
        all_jobs = []

        # Page 1 pour dÃ©terminer le nombre total de pages
        self.logger.info("ğŸ“Š RÃ©cupÃ©ration du nombre total de pages...")
        html_page1 = self.scrape_page(1)

        if not html_page1:
            self.logger.error("âŒ Impossible de charger la premiÃ¨re page")
            return []

        total_pages = self.get_total_pages(html_page1)
        actual_max_pages = min(max_pages or total_pages, total_pages)

        self.logger.info(f"ğŸ“ˆ Total pages: {total_pages}, scraping: {actual_max_pages} pages")

        # Parser la premiÃ¨re page
        jobs_page1 = self.parse_jobs_from_html(html_page1)
        all_jobs.extend(jobs_page1)
        self.logger.info(f"ğŸ“„ Page 1: {len(jobs_page1)} offres trouvÃ©es")

        # Scraper les pages suivantes
        for page in range(2, actual_max_pages + 1):
            self.logger.info(f"ğŸ”„ Page {page}/{actual_max_pages}")

            # DÃ©lai anti-ban
            self.wait_random(delay_min, delay_max)

            html = self.scrape_page(page)
            if not html:
                self.logger.warning(f"âš ï¸ Page {page} ignorÃ©e")
                continue

            jobs = self.parse_jobs_from_html(html)
            all_jobs.extend(jobs)

            self.logger.info(f"ğŸ“„ Page {page}: {len(jobs)} offres trouvÃ©es (Total: {len(all_jobs)})")

        # Statistiques finales
        self.logger.info(f"âœ… Scraping Educarriere terminÃ©: {len(all_jobs)} offres au total")

        return all_jobs


def main():
    """Point d'entrÃ©e pour tests"""
    scraper = EducarriereScraper()

    print("ğŸš€ Test Educarriere Scraper")
    print("=" * 50)

    # Test rapide (2 pages)
    jobs = scraper.run(max_pages=2, delay_min=1.0, delay_max=2.0)

    print(f"\nğŸ“Š RÃ©sultats:")
    print(f"   Offres trouvÃ©es: {len(jobs)}")
    print(f"   EnvoyÃ©es Ã  Kafka: {scraper.stats['jobs_sent_kafka']}")
    print(f"   SauvegardÃ©es MinIO: {scraper.stats['jobs_saved_minio']}")
    print(f"   Erreurs: {scraper.stats['errors']}")

    if jobs:
        print(f"\nğŸ” Exemple d'offre:")
        job = jobs[0]
        print(f"   ID: {job['job_id']}")
        print(f"   Titre: {job['title']}")
        print(f"   Type: {job['job_type']}")
        print(f"   Source: {job['source']}")


if __name__ == '__main__':
    main()
