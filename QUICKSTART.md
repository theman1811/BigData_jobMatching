# âš¡ Guide de DÃ©marrage Rapide

Ce guide vous permet de dÃ©marrer la plateforme BigData en quelques minutes.

## ğŸ“‹ PrÃ©requis

- Docker Desktop installÃ© et lancÃ©
- 8 GB RAM minimum disponible
- 10 GB espace disque

## ğŸš€ DÃ©marrage en 3 Ã©tapes

### Ã‰tape 1 : VÃ©rifier les prÃ©requis

```bash
# Rendre le script exÃ©cutable
chmod +x scripts/setup/check_prerequisites.sh

# ExÃ©cuter la vÃ©rification
./scripts/setup/check_prerequisites.sh
```

### Ã‰tape 2 : DÃ©marrer la plateforme

```bash
# Rendre le script exÃ©cutable
chmod +x start.sh

# DÃ©marrer tous les services
./start.sh
```

â³ **Attendre environ 2-3 minutes** que tous les services dÃ©marrent.

### Ã‰tape 3 : VÃ©rifier que tout fonctionne

```bash
# Rendre le script exÃ©cutable
chmod +x status.sh

# VÃ©rifier le statut
./status.sh
```

## ğŸŒ AccÃ©der aux Interfaces Web

Une fois dÃ©marrÃ©e, la plateforme expose plusieurs interfaces :

| Service | URL | Credentials |
|---------|-----|-------------|
| **Kafka UI** | http://localhost:8080 | Aucun |
| **Spark Master** | http://localhost:8082 | Aucun |
| **Spark Worker 1** | http://localhost:8083 | Aucun |
| **Spark Worker 2** | http://localhost:8084 | Aucun |
| **Airflow** | http://localhost:8085 | user: `airflow`<br>pass: `airflow` |
| **Jupyter** | http://localhost:8888 | token: `bigdata2024` |

## ğŸ“Š VÃ©rifier que les Services Fonctionnent

### Kafka

```bash
# Ouvrir http://localhost:8080
# Vous devriez voir l'interface Kafka UI
```

### Spark

```bash
# Ouvrir http://localhost:8082
# Vous devriez voir le Spark Master avec 2 workers connectÃ©s
```

### Airflow

```bash
# Ouvrir http://localhost:8085
# Login: airflow / airflow
# Vous devriez voir le dashboard Airflow
```

### Jupyter

```bash
# Ouvrir http://localhost:8888
# Token: bigdata2024
# Vous devriez voir l'interface JupyterLab
```

## ğŸ§ª CrÃ©er vos Premiers Topics Kafka

```bash
# Rendre le script exÃ©cutable
chmod +x scripts/setup/create_kafka_topics.sh

# CrÃ©er les topics de test
./scripts/setup/create_kafka_topics.sh
```

## ğŸ“ Tester PySpark dans Jupyter

1. Ouvrir Jupyter : http://localhost:8888 (token: `bigdata2024`)
2. CrÃ©er un nouveau notebook Python
3. Coller ce code de test :

```python
from pyspark.sql import SparkSession

# CrÃ©er une session Spark
spark = SparkSession.builder \
    .appName("Test BigData") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

# CrÃ©er un DataFrame de test
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
df = spark.createDataFrame(data, ["name", "age"])

# Afficher
df.show()

# ArrÃªter
spark.stop()

print("âœ… PySpark fonctionne !")
```

4. ExÃ©cuter (Shift + Enter)
5. VÃ©rifier le rÃ©sultat

## ğŸ›‘ ArrÃªter la Plateforme

```bash
# Rendre le script exÃ©cutable (si pas dÃ©jÃ  fait)
chmod +x stop.sh

# ArrÃªter tous les services
./stop.sh
```

## ğŸ§¹ Nettoyer ComplÃ¨tement (optionnel)

âš ï¸ **Attention** : Ceci supprime toutes les donnÃ©es !

```bash
# Rendre le script exÃ©cutable (si pas dÃ©jÃ  fait)
chmod +x clean.sh

# Nettoyer
./clean.sh
```

## â“ ProblÃ¨mes Courants

### Les conteneurs ne dÃ©marrent pas

**Solution** :
```bash
# VÃ©rifier que Docker est lancÃ©
docker info

# Voir les logs
docker-compose logs -f

# RedÃ©marrer
./stop.sh
./start.sh
```

### "Port already in use"

**Solution** :
```bash
# Trouver le processus qui utilise le port (exemple port 8080)
lsof -i :8080

# ArrÃªter le processus ou changer le port dans docker-compose.yml
```

### Manque de mÃ©moire

**Solution** :
1. Ouvrir Docker Desktop
2. Preferences â†’ Resources
3. Augmenter la RAM Ã  8 GB minimum
4. Apply & Restart

### Airflow ne dÃ©marre pas

**Solution** :
```bash
# Les logs Airflow peuvent prendre 1-2 minutes
# Attendre et vÃ©rifier les logs
docker-compose logs -f airflow-webserver

# Si problÃ¨me de permissions
chmod -R 777 ./airflow/logs ./airflow/dags
./stop.sh
./start.sh
```

## ğŸ“š Prochaines Ã‰tapes

### Phase 1 : Environnement Local âœ…

Vous avez maintenant une plateforme Big Data complÃ¨te qui tourne localement !

### Phase 2 : Configuration GCP

Pour configurer Google Cloud Platform (GCS et BigQuery) :

```bash
# Lire le guide GCP
cat docs/setup_gcp.md

# Ou ouvrir dans votre Ã©diteur
```

### Phase 3 : CrÃ©er vos Pipelines

1. **CrÃ©er un producteur Kafka** dans `kafka/producers/`
2. **CrÃ©er un job Spark** dans `spark/batch/` ou `spark/streaming/`
3. **CrÃ©er un DAG Airflow** dans `airflow/dags/`
4. **Charger dans BigQuery** via Spark
5. **CrÃ©er un dashboard** dans Looker Studio

## ğŸ“– Documentation ComplÃ¨te

- **README.md** : Vue d'ensemble du projet
- **docs/architecture.md** : Architecture dÃ©taillÃ©e
- **docs/setup_gcp.md** : Configuration Google Cloud
- **requirements.txt** : DÃ©pendances Python

## ğŸ†˜ Besoin d'Aide ?

1. VÃ©rifier les logs :
   ```bash
   docker-compose logs -f [nom-du-service]
   ```

2. VÃ©rifier le statut :
   ```bash
   ./status.sh
   ```

3. Consulter la documentation officielle :
   - [Kafka](https://kafka.apache.org/documentation/)
   - [Spark](https://spark.apache.org/docs/latest/)
   - [Airflow](https://airflow.apache.org/docs/)

---

**Bon dÃ©veloppement ! ğŸš€**

