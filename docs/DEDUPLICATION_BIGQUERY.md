# Documentation - D√©duplication BigQuery

## üìã Probl√®me identifi√©

√Ä chaque ex√©cution du DAG `scraping_daily`, les **m√™mes offres d'emploi** √©taient scrap√©es et ins√©r√©es dans BigQuery, causant :

- ‚úÖ **300 offres uniques** constantes
- ‚ùå **Nombre total d'offres en augmentation continue** (doublons)
- üíæ **Gaspillage de stockage et donn√©es dupliqu√©es**

### Cause racine

1. **job_id stable** : Les offres g√©n√®rent toujours le m√™me ID (bas√© sur URL/titre/entreprise, sans timestamp)
2. **Fichiers HTML √©cras√©s** : MinIO √©crase les fichiers avec le m√™me nom (`{job_id}.html`)
3. **Pas de v√©rification des doublons** : Le script `load_to_bigquery.py` utilisait `mode="append"` sans v√©rifier l'existence pr√©alable
4. **D√©duplication d√©sactiv√©e** : La t√¢che Spark de d√©duplication √©tait comment√©e dans le DAG

---

## ‚úÖ Solution impl√©ment√©e : Approche A - D√©duplication Spark avant insertion

### Principe

Avant chaque insertion dans BigQuery, le script Spark :

1. **Lit les IDs existants** depuis la table BigQuery cible
2. **Effectue un LEFT ANTI JOIN** pour filtrer les doublons
3. **Ins√®re uniquement les nouvelles donn√©es**

### Modifications apport√©es

#### Fichier modifi√© : `spark/batch/load_to_bigquery.py`

**Changements pour chaque table** (Fact_OffresEmploi, Dim_Entreprise, Dim_Localisation, Dim_Competence) :

```python
# AVANT (insertion sans v√©rification)
fact_offres_df.write \
    .format("bigquery") \
    .option("table", fact_table) \
    .mode("append") \
    .save()

# APR√àS (d√©duplication avant insertion)
try:
    # Lire les IDs existants
    existing_offres = spark.read \
        .format("bigquery") \
        .option("table", fact_table) \
        .load() \
        .select("offre_id") \
        .distinct()
    
    # Filtrer les nouvelles offres (LEFT ANTI JOIN)
    new_offres = fact_offres_df.join(
        existing_offres,
        on="offre_id",
        how="left_anti"
    )
    
    # Ins√©rer uniquement les nouvelles
    if new_offres.count() > 0:
        new_offres.write \
            .format("bigquery") \
            .option("table", fact_table) \
            .mode("append") \
            .save()
        print(f"‚úÖ {new_offres.count()} nouvelles offres ins√©r√©es")
    else:
        print(f"‚ÑπÔ∏è Aucune nouvelle offre √† ins√©rer")

except Exception as e:
    # Si la table n'existe pas, cr√©er et ins√©rer toutes les donn√©es
    if "Not found: Table" in str(e) or "404" in str(e):
        fact_offres_df.write.format("bigquery").mode("append").save()
```

---

## üìä R√©sultats attendus

### Avant la modification

| Ex√©cution | Offres scrap√©es | Offres ins√©r√©es | Total BigQuery |
|-----------|-----------------|-----------------|----------------|
| Jour 1    | 300             | 300             | 300            |
| Jour 2    | 300             | 300             | 600 ‚ùå         |
| Jour 3    | 300             | 300             | 900 ‚ùå         |

**Probl√®me** : Toujours 300 offres uniques, mais nombre total augmente sans cesse

### Apr√®s la modification

| Ex√©cution | Offres scrap√©es | Nouvelles offres | Offres ins√©r√©es | Total BigQuery |
|-----------|-----------------|------------------|-----------------|----------------|
| Jour 1    | 300             | 300              | 300             | 300            |
| Jour 2    | 300             | 15               | 15              | 315 ‚úÖ         |
| Jour 3    | 300             | 8                | 8               | 323 ‚úÖ         |

**R√©sultat** : Seules les nouvelles offres sont ins√©r√©es, pas de doublons

---

## üöÄ Test et validation

### Comment tester

1. **Vider les tables BigQuery** (si n√©cessaire pour test propre) :
   ```sql
   TRUNCATE TABLE `jobmatching_dw.Fact_OffresEmploi`;
   TRUNCATE TABLE `jobmatching_dw.Dim_Entreprise`;
   TRUNCATE TABLE `jobmatching_dw.Dim_Localisation`;
   TRUNCATE TABLE `jobmatching_dw.Dim_Competence`;
   ```

2. **Ex√©cuter le DAG processing_spark** une premi√®re fois :
   ```bash
   # Depuis Airflow UI ou CLI
   airflow dags trigger processing_spark
   ```

3. **V√©rifier les logs Spark** :
   - Rechercher : `"üìä V√©rification des offres existantes"`
   - Premi√®re ex√©cution : `"0 offres existantes trouv√©es"`
   - Devrait afficher : `"X nouvelles offres √† ins√©rer (sur X au total)"`

4. **Compter les offres dans BigQuery** :
   ```sql
   SELECT COUNT(*) as total_offres,
          COUNT(DISTINCT offre_id) as offres_uniques
   FROM `jobmatching_dw.Fact_OffresEmploi`;
   ```
   ‚Üí **total_offres** doit √©galer **offres_uniques**

5. **R√©ex√©cuter le DAG** (sans nouveau scraping) :
   ```bash
   airflow dags trigger processing_spark
   ```

6. **V√©rifier les logs** :
   - Devrait afficher : `"‚ÑπÔ∏è Aucune nouvelle offre √† ins√©rer"`
   - Ou tr√®s peu de nouvelles offres si scraping entre-temps

7. **Rev√©rifier BigQuery** :
   ```sql
   SELECT COUNT(*) as total_offres,
          COUNT(DISTINCT offre_id) as offres_uniques
   FROM `jobmatching_dw.Fact_OffresEmploi`;
   ```
   ‚Üí **total_offres** doit toujours √©galer **offres_uniques** ‚úÖ

### M√©triques de succ√®s

‚úÖ **`total_offres == offres_uniques`** dans toutes les tables  
‚úÖ **Logs affichent** le nombre de nouvelles vs existantes  
‚úÖ **Pas d'augmentation** du total si aucune nouvelle offre  
‚úÖ **Performance acceptable** (lecture index BigQuery rapide)

---

## üîß Maintenance future

### Nettoyage des anciennes offres

Pour supprimer les offres expir√©es/anciennes (>90 jours) :

```sql
DELETE FROM `jobmatching_dw.Fact_OffresEmploi`
WHERE date_publication < DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY);
```

### Monitoring des doublons

Query de v√©rification mensuelle :

```sql
-- V√©rifier s'il reste des doublons
SELECT offre_id, COUNT(*) as count
FROM `jobmatching_dw.Fact_OffresEmploi`
GROUP BY offre_id
HAVING COUNT(*) > 1
ORDER BY count DESC;
```

---

## üéØ Prochaines √©tapes (optionnelles)

### Option 1 : R√©activer la d√©duplication Spark inter-sources

D√©commenter dans `airflow/dags/processing_spark_dag.py` :

```python
# Lignes 167-171
spark_deduplicate = SparkSubmitOperator(
    task_id='spark_deduplicate',
    application=f"{SPARK_APP_PATH}/deduplicate.py",
    **spark_common_kwargs
)
```

Puis modifier le pipeline pour utiliser `jobs_deduplicated` au lieu de `jobs_parsed`.

**B√©n√©fice** : D√©duplication des offres similaires provenant de sources diff√©rentes (ex: m√™me offre sur Educarriere et Macarrierepro).

### Option 2 : Ajouter une table de tracking

Cr√©er une table `Logs_JobTracking` pour tracer les insertions :

```sql
CREATE TABLE IF NOT EXISTS `jobmatching_dw.Logs_JobTracking` (
  offre_id STRING NOT NULL,
  first_seen TIMESTAMP,
  last_seen TIMESTAMP,
  scraped_count INT64,
  source_sites ARRAY<STRING>
);
```

**B√©n√©fice** : Historique complet de chaque offre (premi√®re apparition, fr√©quence, sources).

---

## üìù Notes techniques

### Performance

- **LEFT ANTI JOIN** : Efficace car utilise les index BigQuery sur la colonne `offre_id`
- **Lecture selective** : Seules les colonnes n√©cessaires sont lues (ex: `offre_id` uniquement)
- **Premi√®re ex√©cution** : Lente (table n'existe pas), ex√©cutions suivantes : rapides

### Limitations

- **Pas d'UPDATE** : Si une offre existe d√©j√†, elle n'est pas mise √† jour (seules les nouvelles sont ins√©r√©es)
- **Co√ªt BigQuery** : Lecture de la table √† chaque ex√©cution (minimis√© par lecture selective)

### Alternatives non retenues

1. **MERGE SQL** : Plus complexe, n√©cessite table staging
2. **Tracking fichiers MinIO** : Ne r√©sout pas le probl√®me des fichiers √©cras√©s
3. **Ajouter timestamp au job_id** : Casse l'unicit√© logique des offres

---

**Date de cr√©ation** : 17 d√©cembre 2024  
**Auteur** : Data Team  
**Version** : 1.0
