# ğŸ“š Documentation Technique - BigData Job Matching

**Date de mise Ã  jour** : DÃ©cembre 2024  
**Version** : 1.0

---

## 5.1. Architecture de la solution

### PrÃ©sentation du systÃ¨me global

Plateforme Big Data hybride (local + cloud) pour l'ingestion, le traitement et l'analyse d'offres d'emploi et de CVs. Architecture modulaire avec orchestration automatisÃ©e via Apache Airflow.

### SchÃ©mas d'architecture

#### Architecture en couches

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COUCHE SCRAPING (Web Scraping)                              â”‚
â”‚ â€¢ Scrapers Python (Educarriere, Macarrierepro, Emploi.ci,   â”‚
â”‚   LinkedIn)                                                 â”‚
â”‚ â€¢ Container Docker dÃ©diÃ©                                     â”‚
â”‚ â€¢ Anti-ban & Rate limiting                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COUCHE INGESTION (Kafka KRaft)                              â”‚
â”‚ â€¢ Kafka Broker (KRaft mode, sans Zookeeper)                  â”‚
â”‚ â€¢ Schema Registry (validation Avro)                          â”‚
â”‚ â€¢ Kafka UI (monitoring)                                      â”‚
â”‚ Topics: job-offers-raw, job-offers-parsed, cvs-raw, etc.    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                           â”‚
         â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MINIO (S3)      â”‚         â”‚ SPARK CLUSTER    â”‚
â”‚ Data Lake       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Processing       â”‚
â”‚ â€¢ scraped-jobs  â”‚         â”‚ â€¢ Master + 2     â”‚
â”‚ â€¢ processed-dataâ”‚         â”‚   Workers        â”‚
â”‚ â€¢ raw-data      â”‚         â”‚ â€¢ Batch +        â”‚
â”‚ â€¢ backups       â”‚         â”‚   Streaming      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COUCHE ORCHESTRATION (Apache Airflow)                        â”‚
â”‚ â€¢ Webserver + Scheduler                                      â”‚
â”‚ â€¢ DAGs: scraping_daily, processing_spark, bigquery_load,     â”‚
â”‚   matching_pipeline                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COUCHE DATA WAREHOUSE (BigQuery - GCP)                      â”‚
â”‚ â€¢ Dataset: jobmatching_dw                                    â”‚
â”‚ â€¢ Tables: Fact_OffresEmploi, Fact_CVs, Dim_*               â”‚
â”‚ â€¢ Vues Superset: v_offres_daily, v_top_competences, etc.   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COUCHE VISUALISATION (Apache Superset)                       â”‚
â”‚ â€¢ Dashboards BI interactifs                                  â”‚
â”‚ â€¢ SQL Lab                                                    â”‚
â”‚ â€¢ Connexion BigQuery                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Modules et interactions

1. **Module Scraping** (`kafka/producers/scrapers/`)
   - Scrapers hÃ©ritant de `BaseJobScraperCI`
   - Publication Kafka + sauvegarde MinIO
   - Rotation User-Agent, dÃ©lais alÃ©atoires

2. **Module Ingestion** (`kafka/`)
   - Topics Kafka pour flux asynchrones
   - Schema Registry pour validation
   - Kafka UI pour monitoring

3. **Module Traitement** (`spark/batch/`, `spark/streaming/`)
   - **Batch** : parsing, extraction, enrichissement
   - **Streaming** : consommation temps rÃ©el depuis Kafka

4. **Module Orchestration** (`airflow/dags/`)
   - DAGs planifiÃ©s avec dÃ©pendances
   - IntÃ©gration Spark, BigQuery, Docker

5. **Module Stockage** (`bigquery/`, MinIO)
   - **MinIO** : donnÃ©es brutes et traitÃ©es (Parquet)
   - **BigQuery** : warehouse avec schÃ©ma en Ã©toile

6. **Module Visualisation** (`config/superset_config.py`)
   - Superset connectÃ© Ã  BigQuery
   - Vues prÃ©-agrÃ©gÃ©es pour dashboards

#### Flux de donnÃ©es

1. **Flux Scraping â†’ Kafka â†’ MinIO**
   ```
   Scrapers â†’ job-offers-raw (Kafka) â†’ MinIO (scraped-jobs/)
   ```

2. **Flux Kafka â†’ Spark Streaming â†’ MinIO**
   ```
   Kafka (job-offers-raw) â†’ Spark Streaming â†’ MinIO (processed-data/jobs/)
   ```

3. **Flux Batch Processing (Airflow)** âš ï¸ **ACTUEL**
   ```
   MinIO (scraped-jobs/*.html) 
     â†’ parse_jobs (lecture directe, BATCH_LIMIT=500)
     â†’ extract_skills 
     â†’ extract_salary 
     â†’ extract_sectors 
     â†’ MinIO (processed-data/)
   ```

4. **Flux Chargement BigQuery**
   ```
   MinIO (processed-data/) â†’ load_to_bigquery â†’ BigQuery (Fact_OffresEmploi)
   ```

5. **Flux Visualisation**
   ```
   BigQuery â†’ Superset â†’ Dashboards interactifs
   ```

---

## 5.2. Conception logique et technique

### Description des composants

#### 1. Scrapers (Classes Python)

**Classe abstraite** : `BaseJobScraperCI` (`kafka/producers/scrapers/base_scraper.py`)
```python
# FonctionnalitÃ©s principales:
- setup_kafka() : Connexion Kafka Producer
- setup_minio() : Connexion MinIO Client
- clean_location_ci() : Normalisation localisations ivoiriennes
- clean_salary_ci() : Parsing salaires FCFA
- _extract_skills_from_text() : Extraction basique compÃ©tences
- create_job_id() : GÃ©nÃ©ration ID dÃ©terministe
```

**Scrapers concrets** :
- `EducarriereScraper` : Scraping Educarriere.ci
- `MacarriereproScraper` : Scraping Macarrierepro.com
- `EmploiCIScraper` : Scraping GoAfricaOnline (remplace Emploi.ci)
- `LinkedInScraper` : Scraping LinkedIn (optionnel, peut Ã©chouer)

#### 2. Jobs Spark Batch

**a) `parse_jobs.py` : Parsing HTML â†’ JSON structurÃ©**
```python
# Configuration actuelle:
- Lecture directe depuis s3a://scraped-jobs/*.html
- Pas d'Ã©tape de merge prÃ©alable (dÃ©sactivÃ©e)
- BATCH_LIMIT=500 fichiers par exÃ©cution
- UDFs d'extraction:
  * extract_title_udf() : Titre depuis HTML
  * extract_company_udf() : Entreprise
  * extract_description_udf() : Description
  * extract_requirements_udf() : Exigences/compÃ©tences
  * extract_location_udf() : Localisation
  * extract_salary_udf() : Salaire
```

**b) `extract_skills.py` : Extraction NLP compÃ©tences**
```python
# Algorithme:
- Catalogue de compÃ©tences par catÃ©gorie (Python, Java, AWS, etc.)
- Recherche pattern-based dans texte
- Classification par catÃ©gorie
- spaCy pour NLP avancÃ© (optionnel)
```

**c) `extract_salary.py` : Extraction salaires**
```python
# Patterns regex:
- Montants FCFA/CFA/XOF
- Montants EUR/$
- PÃ©riode (mois/an/jour)
- Normalisation vers FCFA
```

**d) `extract_sectors.py` : Extraction secteurs d'activitÃ©**
```python
# Classification par secteur:
- Analyse texte description/titre
- Mapping vers secteurs standardisÃ©s
```

**e) `load_to_bigquery.py` : Chargement BigQuery**
```python
# Transformations:
- GÃ©nÃ©ration IDs dÃ©terministes (entreprise_id, localisation_id)
- Mapping vers schÃ©ma BigQuery
- Partitionnement par date_publication
- Clustering par entreprise_id, localisation_id
```

**âš ï¸ Jobs dÃ©sactivÃ©s** :
- `merge_html.py` : **DÃ‰SACTIVÃ‰** - trop lent sur setup local
- `deduplicate.py` : **DÃ‰SACTIVÃ‰** - donnÃ©es d'entrÃ©e trop gÃ©nÃ©riques

#### 3. Pipeline de traitement actuel

```
scraped-jobs/*.html (lecture directe, BATCH_LIMIT=500)
    â†“
spark_parse_jobs (parse_jobs.py)
    â†“
spark_extract_skills (extract_skills.py)
    â†“
spark_extract_salary (extract_salary.py)
    â†“
spark_extract_sectors (extract_sectors.py)
    â†“
check_processing_quality
```

**Note** : Le merge HTML et la dÃ©duplication sont dÃ©sactivÃ©s dans le pipeline actuel (trop lent pour le setup local selon les commentaires du code).

#### 4. SchÃ©ma BigQuery (ModÃ¨le en Ã©toile)

**Tables Dimensions** :
```sql
- Dim_Entreprise (entreprise_id, nom_entreprise, secteur_id, taille_entreprise)
- Dim_Localisation (localisation_id, ville, region, departement, pays, lat/long)
- Dim_Competence (competence_id, nom_competence, categorie, niveau_demande)
- Dim_Secteur (secteur_id, nom_secteur, categorie_parent)
```

**Tables de Fait** :
```sql
- Fact_OffresEmploi (
    offre_id, titre_poste, entreprise_id, localisation_id, secteur_id,
    type_contrat, niveau_experience, teletravail, salaire_min/max,
    competences ARRAY<STRING>, competences_ids ARRAY<STRING>,
    source_site, date_publication, scraped_at
  )
  PARTITION BY date_publication
  CLUSTER BY entreprise_id, localisation_id, secteur_id

- Fact_CVs (
    cv_id, annees_experience, niveau_etudes, competences,
    localisation_souhaitee_id, secteur_souhaite_id, salaire_souhaite
  )
  PARTITION BY DATE(scraped_at)
```

**Tables AgrÃ©gÃ©es** :
```sql
- agg_matching_scores (job_id, candidate_id, match_score, skill_match_pct)
```

**Vues Superset** :
```sql
- v_offres_daily : AgrÃ©gations quotidiennes par source/secteur/localisation
- v_top_competences : Top compÃ©tences avec occurrences
- v_salaires_secteur_ville : Salaires moyens/mÃ©dians par secteur/ville
- v_geo_offres : DonnÃ©es gÃ©ographiques pour cartes
- v_salaires_secteur : Salaires par secteur avec percentiles
- v_teletravail_secteur : Analyse tÃ©lÃ©travail par secteur
```

#### 5. DAGs Airflow

**a) `scraping_daily_dag.py` (02:00 quotidien)**
```python
# TÃ¢ches parallÃ¨les:
- scrape_educarriere (DockerOperator)
- scrape_macarrierepro (DockerOperator)
- scrape_emploi_ci (DockerOperator)
- scrape_linkedin (DockerOperator, optionnel)
â†’ wait_all_scrapers â†’ check_data_quality â†’ notify_completion
```

**b) `processing_spark_dag.py` (04:00 quotidien)**
```python
# Pipeline sÃ©quentiel (sans merge ni dÃ©duplication):
spark_parse_jobs â†’ spark_extract_skills 
â†’ spark_extract_salary â†’ spark_extract_sectors 
â†’ check_processing_quality
```

**c) `bigquery_load_dag.py` (Quotidien)**
```python
# TÃ¢ches:
- check_offers_ready â†’ load_job_offers (SparkSubmitOperator)
- check_cvs_ready â†’ load_cvs_placeholder (EmptyOperator)
```

**d) `matching_pipeline_dag.py` (08:00 quotidien)**
```python
# Pipeline matching (Ã  complÃ©ter):
check_matching_job â†’ spark_matching â†’ load_matching_results 
â†’ generate_recommendations
```

### Interfaces utilisateur / API

1. **Kafka UI** (`http://localhost:8080`)
   - Monitoring topics, consumers, messages
   - Pas d'authentification

2. **MinIO Console** (`http://localhost:9001`)
   - Gestion buckets, fichiers
   - Credentials: `minioadmin` / `minioadmin123`

3. **Spark Web UI**
   - Master: `http://localhost:8082`
   - Worker 1: `http://localhost:8083`
   - Worker 2: `http://localhost:8084`

4. **Airflow UI** (`http://localhost:8085`)
   - Monitoring DAGs, tÃ¢ches, logs
   - Credentials: `airflow` / `airflow`

5. **Superset** (`http://localhost:8088`)
   - Dashboards, SQL Lab
   - Credentials: `admin` / `admin`

6. **Jupyter** (`http://localhost:8888`)
   - Notebooks PySpark
   - Token: `bigdata2024`

---

## 5.3. Mise en Å“uvre pratique

### Description des Ã©tapes de dÃ©veloppement / intÃ©gration

#### Phase 1 : Configuration initiale

1. **PrÃ©requis**
   ```bash
   - Docker Desktop installÃ©
   - Python 3.11+
   - Compte GCP (pour BigQuery)
   - 10 GB RAM minimum
   ```

2. **Configuration GCP**
   ```bash
   # CrÃ©er Service Account
   gcloud iam service-accounts create bigdata-sa
   
   # Permissions BigQuery
   gcloud projects add-iam-policy-binding PROJECT_ID \
     --member="serviceAccount:bigdata-sa@PROJECT_ID.iam.gserviceaccount.com" \
     --role="roles/bigquery.dataEditor"
   
   # TÃ©lÃ©charger clÃ© JSON
   gcloud iam service-accounts keys create ./credentials/gcp-service-account.json \
     --iam-account=bigdata-sa@PROJECT_ID.iam.gserviceaccount.com
   ```

3. **Variables d'environnement**
   ```bash
   # CrÃ©er .env
   GCP_PROJECT_ID=your-project-id
   BIGQUERY_DATASET=jobmatching_dw
   ```

#### Phase 2 : DÃ©marrage infrastructure

1. **Build images Docker**
   ```bash
   docker-compose build
   ```

2. **DÃ©marrer services**
   ```bash
   ./start.sh
   # Ou: docker-compose up -d
   ```

3. **VÃ©rification services**
   ```bash
   ./status.sh
   # VÃ©rifier que tous les conteneurs sont "Up"
   ```

#### Phase 3 : Initialisation BigQuery

1. **CrÃ©er dataset**
   ```bash
   bq mk --dataset jobmatching_dw
   ```

2. **CrÃ©er tables**
   ```bash
   bq query --use_legacy_sql=false < bigquery/schemas/create_tables.sql
   ```

3. **CrÃ©er vues Superset**
   ```bash
   bq query --use_legacy_sql=false < bigquery/queries/superset_views.sql
   ```

#### Phase 4 : Configuration Superset

1. **Connexion BigQuery**
   - UI Superset â†’ Databases â†’ Add Database
   - Connection String: `bigquery://PROJECT_ID/?credentials_path=/opt/airflow/credentials/bq-service-account.json`

2. **Publier datasets**
   - Importer tables depuis BigQuery
   - Publier vues: `v_offres_daily`, `v_top_competences`, etc.

3. **CrÃ©er dashboards**
   - MarchÃ© de l'Emploi (offres/jour, top compÃ©tences, carte)
   - Tendances Salariales (Ã©volution, comparaisons)
   - Analyse CompÃ©tences (Ã©mergentes, combinaisons)

#### Phase 5 : Tests et validation

1. **Test scraping manuel**
   ```bash
   docker exec -it bigdata_scrapers python /app/producers/run_scraper.py \
     --scraper educarriere --max-pages 2
   ```

2. **VÃ©rification Kafka**
   ```bash
   # Kafka UI: http://localhost:8080
   # VÃ©rifier messages dans job-offers-raw
   ```

3. **Test Spark job**
   ```bash
   # Via Airflow UI: dÃ©clencher DAG processing_spark manuellement
   # Ou via Jupyter: exÃ©cuter parse_jobs.py
   ```

4. **VÃ©rification BigQuery**
   ```bash
   bq query --use_legacy_sql=false \
     "SELECT COUNT(*) FROM jobmatching_dw.Fact_OffresEmploi"
   ```

### Environnements de dÃ©ploiement

#### Environnement local (Docker Compose)

**Services conteneurisÃ©s** (17 conteneurs) :
- Kafka (KRaft), Schema Registry, Kafka UI
- MinIO (Data Lake S3)
- Spark (Master + 2 Workers)
- Airflow (Webserver + Scheduler + Init)
- Superset (+ Init)
- PostgreSQL (Airflow + Superset)
- Redis (Cache)
- Jupyter
- Scrapers

**Volumes persistants** :
```yaml
- kafka_data
- minio_data
- spark_master_data, spark_worker_1_data, spark_worker_2_data
- postgres_data
- redis_data
- superset_home
```

**RÃ©seau** : `bigdata_network` (bridge)

#### Environnement cloud (GCP)

**BigQuery (Free Tier)** :
- 10 GB stockage gratuit
- 1 TB requÃªtes/mois gratuit
- Dataset: `jobmatching_dw`
- Tables partitionnÃ©es et clusterisÃ©es

**Service Account** :
- Authentification via JSON key
- Permissions: `roles/bigquery.dataEditor`
- MontÃ© dans conteneurs Airflow/Spark

#### Configuration hybride

**Local** :
- Scraping, Kafka, Spark, MinIO, Airflow, Superset
- DÃ©veloppement et tests

**Cloud** :
- BigQuery uniquement (warehouse)
- Visualisation Superset connectÃ©e Ã  BigQuery

#### Scripts de gestion

```bash
./start.sh      # DÃ©marre tous les services
./stop.sh       # ArrÃªte tous les services
./status.sh     # Affiche statut des conteneurs
./clean.sh      # Supprime volumes (âš ï¸ perte de donnÃ©es)
```

#### Monitoring et logs

- **Kafka UI** : monitoring topics
- **Spark UI** : jobs en cours
- **Airflow UI** : Ã©tat DAGs, logs tÃ¢ches
- **MinIO Console** : fichiers stockÃ©s
- **Superset** : mÃ©triques dashboards
- **Logs Docker** : `docker logs <container_name>`

---

## RÃ©sumÃ© technique

- **Architecture** : 6 couches (Scraping â†’ Ingestion â†’ Traitement â†’ Stockage â†’ Orchestration â†’ Visualisation)
- **Technologies** : Kafka KRaft, Spark 3.5.1, Airflow 2.8.0, Superset, MinIO, BigQuery
- **ModÃ¨le de donnÃ©es** : SchÃ©ma en Ã©toile (BigQuery) avec partitionnement et clustering
- **Orchestration** : 4 DAGs Airflow avec dÃ©pendances et planification
- **Pipeline actuel** : Parse â†’ Extract Skills â†’ Extract Salary â†’ Extract Sectors (sans merge ni dÃ©duplication)
- **DÃ©ploiement** : Hybride (local Docker + cloud BigQuery)
- **CoÃ»t** : 0â‚¬ (Free Tier GCP suffisant pour dÃ©veloppement)

Cette architecture permet un pipeline automatisÃ© de bout en bout, du scraping Ã  la visualisation, avec scalabilitÃ© et monitoring intÃ©grÃ©s.

---

## Notes importantes

âš ï¸ **Jobs dÃ©sactivÃ©s** :
- `merge_html.py` : DÃ©sactivÃ© car trop lent sur setup local
- `deduplicate.py` : DÃ©sactivÃ© car donnÃ©es d'entrÃ©e trop gÃ©nÃ©riques

âœ… **Pipeline actuel optimisÃ©** :
- Lecture directe depuis `scraped-jobs/*.html`
- Limite de 500 fichiers par exÃ©cution (`BATCH_LIMIT`)
- Pas d'Ã©tape de merge prÃ©alable
- Pipeline simplifiÃ© pour performance locale
