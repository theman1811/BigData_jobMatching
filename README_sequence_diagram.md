# ğŸ“Š Diagramme de SÃ©quences UML - Pipeline Big Data Job Matching

## ğŸ¯ Vue d'ensemble

Ce document explique le diagramme de sÃ©quences UML (`sequence_diagram.xml`) qui dÃ©crit le pipeline complet de traitement des donnÃ©es Big Data pour le projet Job Matching.

### Objectif
Le diagramme illustre la **sÃ©quence temporelle des interactions** entre les diffÃ©rents composants du systÃ¨me Big Data, depuis le dÃ©clenchement du scraping jusqu'Ã  la visualisation des donnÃ©es.

---

## ğŸ—ï¸ Composants reprÃ©sentÃ©s (Lifelines)

Chaque ligne verticale reprÃ©sente un **composant actif** du systÃ¨me :

| Composant | RÃ´le | Couleur |
|-----------|------|---------|
| **Data Analyst** | Utilisateur dÃ©clenchant les processus | ğŸŸ¡ Jaune |
| **Airflow DAG** | Orchestrateur des pipelines | ğŸŸ£ Violet |
| **Scraper Service** | Service de web scraping | ğŸ”´ Rouge |
| **Kafka Broker** | Bus de messages (streaming) | ğŸ”µ Bleu clair |
| **Spark Streaming Job** | Traitement temps rÃ©el des donnÃ©es | ğŸŸ¢ Vert menthe |
| **MinIO Client** | Stockage objet (Data Lake) | ğŸŸ¢ Turquoise |
| **BigQuery Loader** | Chargement vers Data Warehouse | ğŸŸ  Orange |
| **Superset Dashboard** | Interface de visualisation BI | âšª Gris |

---

## ğŸ”„ SÃ©quence des interactions

### Phase 1 : DÃ©clenchement (Y=200)
```
Data Analyst â†’ Airflow DAG : triggerScrapingDAG()
    â†“
Airflow DAG â†’ Scraper Service : startScraping()
```

**Explication** : L'analyste de donnÃ©es dÃ©clenche manuellement un DAG Airflow qui lance le processus de scraping.

### Phase 2 : Ingestion (Y=280-380)
```
Scraper Service â†’ Kafka Broker : <<async>> publish(job-offers-raw, cvs-raw)
Scraper Service â†’ MinIO Client : saveRawFiles(html, pdf)
```

**Explication** : Le scraper collecte les donnÃ©es des sites web et les publie simultanÃ©ment dans Kafka (pour traitement temps rÃ©el) et MinIO (pour archivage brut).

### Phase 3 : Traitement (Y=420-600)
```
Kafka Broker â†’ Spark Streaming Job : <<async stream>> consume(job-offers-raw, cvs-raw)
Spark Streaming Job â†’ Spark Streaming Job : processNLP(text, skills)
Spark Streaming Job â†’ MinIO Client : saveProcessedData(parquet)
Spark Streaming Job â†’ Kafka Broker : <<async>> publish(job-offers-parsed, cvs-parsed)
```

**Explication** : Spark consomme en continu les messages Kafka, applique le traitement NLP (extraction de compÃ©tences, normalisation), sauvegarde les donnÃ©es structurÃ©es dans MinIO, puis republie les donnÃ©es enrichies dans Kafka.

### Phase 4 : Chargement (Y=720-750)
```
Airflow DAG â†’ BigQuery Loader : loadBatchData()
BigQuery Loader â†’ MinIO Client : readProcessedData()
```

**Explication** : Selon un planning (DAG quotidien), Airflow dÃ©clenche le chargement batch des donnÃ©es traitÃ©es depuis MinIO vers BigQuery.

### Phase 5 : Visualisation (Y=850-890)
```
Data Analyst â†’ Superset Dashboard : createDashboard()
Superset Dashboard â†’ BigQuery Loader : <<async query>> SELECT * FROM fact_jobs, dim_skills
```

**Explication** : L'analyste crÃ©e des tableaux de bord dans Superset qui interrogent directement BigQuery pour afficher les analyses temps rÃ©el.

---

## ğŸ“¨ Types de messages

### ğŸ”µ Messages synchrones (appels de mÃ©thodes)
- **FlÃ¨che pleine** avec retour en pointillÃ©
- Attendent une rÃ©ponse avant de continuer
- Exemples : `triggerScrapingDAG()`, `startScraping()`, `saveRawFiles()`

### ğŸŸ  Messages asynchrones (Ã©vÃ©nements)
- **FlÃ¨che ouverte pointillÃ©e** sans retour obligatoire
- Ne bloquent pas l'exÃ©cution
- Types :
  - `<<async>>` : Publication/consommation simple
  - `<<async stream>>` : Traitement en continu
  - `<<async query>>` : RequÃªtes de donnÃ©es

---

## ğŸ“ Conventions UML utilisÃ©es

### Lignes de vie (Lifelines)
- **Trait vertical pointillÃ©** : ReprÃ©sente l'existence temporelle du composant
- **BoÃ®te d'activation** : Rectangle fin montrant la pÃ©riode d'activitÃ©

### StÃ©rÃ©otypes
- `<<async>>` : Message asynchrone
- `<<async stream>>` : Streaming continu
- `<<async query>>` : RequÃªte de donnÃ©es

### Couleurs
- **Bleu (#1ba1e2)** : Messages synchrones (appels)
- **Orange (#ff9900)** : Messages asynchrones (Ã©vÃ©nements)
- **Gris (#666666)** : Messages de retour

---

## ğŸ”§ Comment utiliser le diagramme

### Ouverture dans draw.io
1. Aller sur [app.diagrams.net](https://app.diagrams.net)
2. **Fichier â†’ Ouvrir** â†’ SÃ©lectionner `sequence_diagram.xml`
3. Le diagramme s'affiche avec toutes les interactions

### Lecture du diagramme
1. **Commencer par le haut** : Les interactions se dÃ©roulent de haut en bas
2. **Suivre les flÃ¨ches** : Chaque flÃ¨che reprÃ©sente un message entre composants
3. **Regarder les couleurs** : Bleu = synchrone, Orange = asynchrone
4. **Observer les boÃ®tes** : Les rectangles fins montrent quand chaque composant est actif

### Points d'attention
- **Kafka** : Sert de bus de messages asynchrone entre scraper et Spark
- **MinIO** : Stockage persistant (Data Lake) pour toutes les donnÃ©es
- **BigQuery** : Data Warehouse pour les requÃªtes analytiques
- **Airflow** : Orchestrateur qui dÃ©clenche les processus batch

---

## ğŸ”„ Flux de donnÃ©es rÃ©sumÃ©

```
Sites Web â†’ Scraper â†’ Kafka â†’ Spark â†’ MinIO â†’ Airflow â†’ BigQuery â†’ Superset
(Indeed,    (Service)   (Bus)   (NLP)   (Lake)    (Batch)    (DW)      (BI)
 LinkedIn,
 etc.)
```

### DonnÃ©es collectÃ©es
- **Offres d'emploi** : Titre, entreprise, salaire, compÃ©tences, localisation
- **CVs candidats** : CompÃ©tences, expÃ©rience, prÃ©tentions salariales

### Transformations appliquÃ©es
- **Parsing HTML/PDF** â†’ Extraction structurÃ©e
- **NLP** â†’ Normalisation compÃ©tences, entitÃ©s nommÃ©es
- **DÃ©doublonnage** â†’ Ã‰limination des doublons
- **Enrichissement** â†’ Calcul scores de matching

---

## ğŸ“š RÃ©fÃ©rences

- [Standard UML Sequence Diagrams](https://www.uml.org/)
- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Apache Spark Documentation](https://spark.apache.org/docs/)
- [Google BigQuery](https://cloud.google.com/bigquery)
- [Apache Superset](https://superset.apache.org/)

---

*DerniÃ¨re mise Ã  jour : Novembre 2024*
