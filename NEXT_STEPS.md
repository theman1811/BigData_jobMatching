# ðŸš€ Prochaines Ã‰tapes - Votre Plan d'Action

## âœ… Ce qui est Fait

Votre infrastructure Big Data est **100% configurÃ©e** :

### Infrastructure
- âœ… Docker Compose avec 17 services
- âœ… Kafka KRaft (sans Zookeeper)
- âœ… MinIO (Data Lake S3)
- âœ… Apache Spark (1 Master + 2 Workers)
- âœ… Apache Airflow
- âœ… Apache Superset
- âœ… PostgreSQL + Redis
- âœ… Jupyter avec PySpark
- âœ… Container Scrapers

### Configuration
- âœ… Tous les fichiers de config crÃ©Ã©s
- âœ… Scripts de dÃ©marrage/arrÃªt
- âœ… Documentation complÃ¨te
- âœ… Requirements Python

### Documentation
- âœ… README.md complet
- âœ… Architecture dÃ©taillÃ©e
- âœ… Guides de dÃ©marrage
- âœ… Changelog

## ðŸŽ¯ Ce qu'il Reste Ã  Faire

### Phase 1 : DÃ©marrage et Tests (1-2 jours)

#### 1.1 DÃ©marrer la plateforme

```bash
# Copier .env
cp .env.example .env

# DÃ©marrer tous les services
./start.sh

# Attendre 3-4 minutes
# VÃ©rifier le statut
./status.sh
```

**Checklist** :
- [ ] Tous les conteneurs dÃ©marrent sans erreur
- [ ] Kafka UI accessible (http://localhost:8080)
- [ ] MinIO accessible (http://localhost:9001)
- [ ] Spark Master accessible (http://localhost:8082)
- [ ] Airflow accessible (http://localhost:8085)
- [ ] Superset accessible (http://localhost:8088)
- [ ] Jupyter accessible (http://localhost:8888)

#### 1.2 Tests basiques

**Test Kafka** :
```bash
# CrÃ©er un topic
docker exec -it bigdata_kafka kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --topic test-job-offers \
  --partitions 3
```

**Test MinIO** :
- Ouvrir http://localhost:9001
- Login : minioadmin / minioadmin123
- VÃ©rifier que les 6 buckets existent

**Test Spark + MinIO** :
- Ouvrir Jupyter : http://localhost:8888
- ExÃ©cuter le code de test dans `QUICKSTART_NEW.md`

**RÃ©sultat attendu** :
- [ ] Kafka fonctionne
- [ ] MinIO fonctionne
- [ ] Spark lit/Ã©crit dans MinIO
- [ ] Pas d'erreurs dans les logs

### Phase 2 : Configuration BigQuery (1 jour)

#### 2.1 CrÃ©er le projet GCP

1. Aller sur https://console.cloud.google.com
2. CrÃ©er un nouveau projet : `job-matching-bigdata`
3. Activer BigQuery API
4. CrÃ©er un dataset : `job_matching_dw`

#### 2.2 CrÃ©er le Service Account

```bash
# Installer gcloud CLI si pas dÃ©jÃ  fait
# https://cloud.google.com/sdk/docs/install

# Se connecter
gcloud auth login

# CrÃ©er le service account
gcloud iam service-accounts create bigdata-sa \
  --display-name="BigData Service Account" \
  --project=job-matching-bigdata

# Donner les permissions BigQuery
gcloud projects add-iam-policy-binding job-matching-bigdata \
  --member="serviceAccount:bigdata-sa@job-matching-bigdata.iam.gserviceaccount.com" \
  --role="roles/bigquery.dataEditor"

# CrÃ©er la clÃ© JSON
gcloud iam service-accounts keys create ./config/gcp-service-account.json \
  --iam-account=bigdata-sa@job-matching-bigdata.iam.gserviceaccount.com
```

#### 2.3 Configurer les variables

```bash
# Ã‰diter .env
nano .env

# Ajouter :
GCP_PROJECT_ID=job-matching-bigdata
GCP_DATASET_ID=job_matching_dw
GCP_SERVICE_ACCOUNT_KEY_PATH=/opt/airflow/config/gcp-service-account.json
```

#### 2.4 CrÃ©er les tables BigQuery

ExÃ©cuter dans la console BigQuery :

```sql
-- Table des offres d'emploi
CREATE TABLE job_matching_dw.fact_job_offers (
  job_id STRING NOT NULL,
  title STRING,
  company_id STRING,
  location_id STRING,
  description TEXT,
  salary_min FLOAT64,
  salary_max FLOAT64,
  contract_type STRING,
  remote_option BOOLEAN,
  skills ARRAY<STRING>,
  posted_date DATE,
  scraped_at TIMESTAMP,
  source STRING
)
PARTITION BY posted_date
CLUSTER BY company_id, location_id;

-- Table des CVs
CREATE TABLE job_matching_dw.fact_candidates (
  candidate_id STRING NOT NULL,
  skills ARRAY<STRING>,
  years_experience INT64,
  education_level STRING,
  desired_salary FLOAT64,
  location_id STRING,
  scraped_at TIMESTAMP
)
PARTITION BY DATE(scraped_at);

-- Table des compÃ©tences
CREATE TABLE job_matching_dw.dim_skills (
  skill_id STRING NOT NULL,
  skill_name STRING,
  skill_category STRING,
  created_at TIMESTAMP
);

-- Table des entreprises
CREATE TABLE job_matching_dw.dim_companies (
  company_id STRING NOT NULL,
  company_name STRING,
  industry STRING,
  size STRING,
  location STRING,
  website STRING
);

-- Table des matching
CREATE TABLE job_matching_dw.agg_matching_scores (
  job_id STRING,
  candidate_id STRING,
  match_score FLOAT64,
  skill_match_pct FLOAT64,
  salary_match_pct FLOAT64,
  location_match_pct FLOAT64,
  calculated_at TIMESTAMP
)
PARTITION BY DATE(calculated_at);
```

**Checklist** :
- [ ] Projet GCP crÃ©Ã©
- [ ] Service Account crÃ©Ã©
- [ ] ClÃ© JSON tÃ©lÃ©chargÃ©e
- [ ] Variables .env configurÃ©es
- [ ] Tables BigQuery crÃ©Ã©es

### Phase 3 : ImplÃ©menter les Scrapers (3-5 jours)

#### 3.1 CrÃ©er la structure

```bash
mkdir -p kafka/producers/scrapers
mkdir -p kafka/producers/utils
```

#### 3.2 Scraper de base

CrÃ©er `kafka/producers/scrapers/base_scraper.py` :

```python
from abc import ABC, abstractmethod
from kafka import KafkaProducer
from minio import Minio
import json
import logging

class BaseJobScraper(ABC):
    """Classe abstraite pour tous les scrapers"""
    
    def __init__(self, kafka_servers, minio_endpoint):
        self.kafka_producer = KafkaProducer(
            bootstrap_servers=kafka_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        self.minio_client = Minio(
            minio_endpoint,
            access_key="minioadmin",
            secret_key="minioadmin123",
            secure=False
        )
        self.logger = logging.getLogger(self.__class__.__name__)
    
    @abstractmethod
    def scrape_page(self, url):
        """Ã€ implÃ©menter : scraper une page"""
        pass
    
    @abstractmethod
    def parse_job(self, html):
        """Ã€ implÃ©menter : parser une offre"""
        pass
    
    def send_to_kafka(self, job_data, topic='job-offers-raw'):
        """Envoyer Ã  Kafka"""
        self.kafka_producer.send(topic, job_data)
        self.logger.info(f"Job sent to Kafka: {job_data.get('job_id')}")
    
    def save_to_minio(self, job_id, html_content, bucket='scraped-jobs'):
        """Sauvegarder HTML dans MinIO"""
        from io import BytesIO
        data = BytesIO(html_content.encode('utf-8'))
        self.minio_client.put_object(
            bucket,
            f"{job_id}.html",
            data,
            length=len(html_content)
        )
        self.logger.info(f"HTML saved to MinIO: {job_id}")
```

#### 3.3 Scraper Indeed

CrÃ©er `kafka/producers/scrapers/indeed_scraper.py` :

```python
import requests
from bs4 import BeautifulSoup
from .base_scraper import BaseJobScraper
import time
import random

class IndeedScraper(BaseJobScraper):
    """Scraper pour Indeed France"""
    
    BASE_URL = "https://fr.indeed.com"
    
    def scrape_page(self, keyword, location, page=0):
        """Scraper une page de rÃ©sultats"""
        url = f"{self.BASE_URL}/jobs"
        params = {
            'q': keyword,
            'l': location,
            'start': page * 10
        }
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, params=params, headers=headers)
        return response.text
    
    def parse_job(self, html):
        """Parser les offres d'une page"""
        soup = BeautifulSoup(html, 'html.parser')
        jobs = []
        
        for job_card in soup.find_all('div', class_='job_seen_beacon'):
            try:
                job_data = {
                    'job_id': job_card.get('data-jk', ''),
                    'title': job_card.find('h2').text.strip(),
                    'company': job_card.find('span', class_='companyName').text.strip(),
                    'location': job_card.find('div', class_='companyLocation').text.strip(),
                    'source': 'indeed',
                    'scraped_at': time.time()
                }
                jobs.append(job_data)
            except Exception as e:
                self.logger.error(f"Error parsing job: {e}")
        
        return jobs
    
    def scrape(self, keyword, location, max_pages=5):
        """Scraper plusieurs pages"""
        all_jobs = []
        
        for page in range(max_pages):
            self.logger.info(f"Scraping page {page+1}/{max_pages}")
            
            # Scrape
            html = self.scrape_page(keyword, location, page)
            jobs = self.parse_job(html)
            
            # Envoyer Ã  Kafka et MinIO
            for job in jobs:
                self.send_to_kafka(job)
                self.save_to_minio(job['job_id'], html)
                all_jobs.append(job)
            
            # Rate limiting
            time.sleep(random.uniform(2, 5))
        
        return all_jobs
```

#### 3.4 Utilisation

CrÃ©er `kafka/producers/run_scraper.py` :

```python
from scrapers.indeed_scraper import IndeedScraper
import os

if __name__ == '__main__':
    scraper = IndeedScraper(
        kafka_servers=os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:29092'),
        minio_endpoint=os.getenv('MINIO_ENDPOINT', 'http://minio:9000')
    )
    
    jobs = scraper.scrape(
        keyword='data engineer',
        location='paris',
        max_pages=5
    )
    
    print(f"âœ… {len(jobs)} offres scrapÃ©es")
```

**Checklist** :
- [ ] Base scraper crÃ©Ã©
- [ ] Indeed scraper implÃ©mentÃ©
- [ ] Tests locaux rÃ©ussis
- [ ] DonnÃ©es dans Kafka
- [ ] HTML dans MinIO

**Prochains scrapers Ã  implÃ©menter** :
- [ ] LinkedIn (`linkedin_scraper.py`)
- [ ] Welcome to the Jungle (`wttj_scraper.py`)
- [ ] Apec (`apec_scraper.py`)

### Phase 4 : Jobs Spark (3-5 jours)

#### 4.1 Parser les offres (Spark Streaming)

CrÃ©er `spark/streaming/consume_jobs.py` :

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

spark = SparkSession.builder \
    .appName("ConsumeJobs") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .getOrCreate()

# Lire depuis Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:29092") \
    .option("subscribe", "job-offers-raw") \
    .load()

# Parser le JSON
schema = StructType([
    StructField("job_id", StringType()),
    StructField("title", StringType()),
    StructField("company", StringType()),
    StructField("location", StringType()),
    StructField("source", StringType()),
    StructField("scraped_at", DoubleType())
])

parsed_df = df.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")

# Ajouter des transformations
clean_df = parsed_df \
    .withColumn("scraped_date", from_unixtime("scraped_at").cast("date")) \
    .withColumn("title_clean", lower(col("title")))

# Ã‰crire dans MinIO (Parquet)
query = clean_df.writeStream \
    .format("parquet") \
    .option("path", "s3a://processed-data/jobs/") \
    .option("checkpointLocation", "s3a://processed-data/checkpoints/jobs/") \
    .partitionBy("scraped_date", "source") \
    .outputMode("append") \
    .start()

query.awaitTermination()
```

#### 4.2 Extraction NLP

CrÃ©er `spark/batch/extract_skills.py` :

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType
import spacy

# Charger modÃ¨le spaCy
nlp = spacy.load("fr_core_news_md")

def extract_skills(text):
    """Extraire les compÃ©tences techniques"""
    if not text:
        return []
    
    doc = nlp(text)
    skills = []
    
    # Liste de compÃ©tences techniques (Ã  enrichir)
    tech_skills = {
        'python', 'java', 'spark', 'kafka', 'sql', 'hadoop',
        'docker', 'kubernetes', 'aws', 'gcp', 'azure',
        'machine learning', 'data science', 'big data'
    }
    
    # Extraire les tokens qui matchent
    for token in doc:
        if token.text.lower() in tech_skills:
            skills.append(token.text)
    
    return list(set(skills))

# CrÃ©er UDF
extract_skills_udf = udf(extract_skills, ArrayType(StringType()))

# Utilisation
spark = SparkSession.builder.appName("ExtractSkills").getOrCreate()

df = spark.read.parquet("s3a://processed-data/jobs/")

df_with_skills = df.withColumn("skills", extract_skills_udf(col("description")))

df_with_skills.write \
    .mode("overwrite") \
    .parquet("s3a://processed-data/jobs_enriched/")
```

**Checklist** :
- [ ] Spark Streaming configurÃ©
- [ ] Parser JSON â†’ Parquet
- [ ] Extraction NLP fonctionnelle
- [ ] DÃ©duplication
- [ ] Matching offres-CVs

### Phase 5 : Airflow DAGs (2-3 jours)

#### 5.1 DAG de scraping quotidien

CrÃ©er `airflow/dags/scraping_daily.py` :

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'bigdata',
    'depends_on_past': False,
    'start_date': datetime(2024, 11, 24),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'scraping_daily',
    default_args=default_args,
    description='Scraping quotidien des offres',
    schedule_interval='0 2 * * *',  # 2h du matin
    catchup=False
)

# Scraper Indeed
scrape_indeed = BashOperator(
    task_id='scrape_indeed',
    bash_command='python /opt/airflow/kafka/producers/run_scraper.py --site indeed',
    dag=dag
)

# Scraper LinkedIn
scrape_linkedin = BashOperator(
    task_id='scrape_linkedin',
    bash_command='python /opt/airflow/kafka/producers/run_scraper.py --site linkedin',
    dag=dag
)

# Traiter avec Spark
process_jobs = SparkSubmitOperator(
    task_id='process_jobs',
    application='/opt/airflow/spark/streaming/consume_jobs.py',
    conn_id='spark_default',
    dag=dag
)

# Charger dans BigQuery
load_to_bq = BashOperator(
    task_id='load_to_bigquery',
    bash_command='python /opt/airflow/scripts/load_to_bigquery.py',
    dag=dag
)

# DÃ©finir l'ordre
[scrape_indeed, scrape_linkedin] >> process_jobs >> load_to_bq
```

**Checklist** :
- [ ] DAG de scraping crÃ©Ã©
- [ ] DAG de processing crÃ©Ã©
- [ ] DAG de loading vers BigQuery crÃ©Ã©
- [ ] Tests sur Airflow UI

### Phase 6 : Dashboards Superset (2 jours)

#### 6.1 Connecter BigQuery

1. Ouvrir http://localhost:8088
2. Settings â†’ Database Connections
3. Add Database
4. Choisir BigQuery
5. Config :
   ```
   bigquery://job-matching-bigdata/job_matching_dw?credentials_path=/app/gcp-service-account.json
   ```

#### 6.2 CrÃ©er les datasets

- Dataset 1 : `fact_job_offers`
- Dataset 2 : `fact_candidates`
- Dataset 3 : `agg_matching_scores`

#### 6.3 CrÃ©er les dashboards

**Dashboard 1 : MarchÃ© de l'Emploi**
- Chart 1 : Offres par jour (Line chart)
- Chart 2 : Top 10 compÃ©tences (Bar chart)
- Chart 3 : Carte gÃ©ographique (Map)
- Chart 4 : Salaires moyens (Box plot)

**Dashboard 2 : Analyse Candidats**
- Chart 1 : Distribution expÃ©rience (Histogram)
- Chart 2 : CompÃ©tences recherchÃ©es (Word cloud)
- Chart 3 : Matching scores (Scatter plot)

**Checklist** :
- [ ] Connexion BigQuery configurÃ©e
- [ ] Datasets crÃ©Ã©s
- [ ] 2+ dashboards fonctionnels
- [ ] Rapports planifiÃ©s

### Phase 7 : Tests et Documentation (1-2 jours)

- [ ] Tests end-to-end
- [ ] Documentation des scrapers
- [ ] Documentation des jobs Spark
- [ ] Documentation des DAGs
- [ ] Tutoriels vidÃ©o (optionnel)

## ðŸ“Š Estimation Totale

| Phase | DurÃ©e | PrioritÃ© |
|-------|-------|----------|
| 1. DÃ©marrage et tests | 1-2 jours | ðŸ”´ Critique |
| 2. BigQuery | 1 jour | ðŸ”´ Critique |
| 3. Scrapers | 3-5 jours | ðŸŸ  Haute |
| 4. Jobs Spark | 3-5 jours | ðŸŸ  Haute |
| 5. DAGs Airflow | 2-3 jours | ðŸŸ¡ Moyenne |
| 6. Dashboards | 2 jours | ðŸŸ¡ Moyenne |
| 7. Tests | 1-2 jours | ðŸŸ¢ Basse |
| **TOTAL** | **13-20 jours** | |

## ðŸŽ¯ Objectifs par Semaine

### Semaine 1 (Jours 1-5)
- âœ… Infrastructure dÃ©marrÃ©e
- âœ… BigQuery configurÃ©
- âœ… Premier scraper fonctionnel
- âœ… DonnÃ©es dans Kafka + MinIO

### Semaine 2 (Jours 6-10)
- âœ… Tous les scrapers implÃ©mentÃ©s
- âœ… Jobs Spark de parsing
- âœ… Extraction NLP basique
- âœ… DonnÃ©es dans BigQuery

### Semaine 3 (Jours 11-15)
- âœ… DAGs Airflow complets
- âœ… Pipeline automatisÃ©
- âœ… Dashboards Superset
- âœ… Documentation

### Semaine 4 (Jours 16-20)
- âœ… Tests et optimisations
- âœ… PrÃ©sentation
- âœ… Livrables finaux

## ðŸ“ž Support

**Besoin d'aide ?**
- Documentation : `docs/`
- Exemples : `notebooks/exploration/`
- Logs : `docker logs [service]`

**Ressources utiles** :
- Scrapy Tutorial : https://docs.scrapy.org/en/latest/intro/tutorial.html
- Spark Streaming : https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html
- Airflow : https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html

---

**ðŸš€ Bon courage ! Vous avez tout ce qu'il faut pour rÃ©ussir !**

