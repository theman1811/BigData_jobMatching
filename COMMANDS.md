# ðŸŽ® Aide-MÃ©moire des Commandes

Guide de rÃ©fÃ©rence rapide pour toutes les commandes disponibles dans ce projet.

## ðŸš€ Gestion de la Plateforme

### DÃ©marrage

```bash
# DÃ©marrer tous les services
./start.sh

# DÃ©marrer en mode dÃ©tachÃ© (sans logs)
docker-compose up -d

# DÃ©marrer un service spÃ©cifique
docker-compose up -d kafka
docker-compose up -d spark-master
docker-compose up -d airflow-webserver
```

### ArrÃªt

```bash
# ArrÃªter tous les services
./stop.sh

# ArrÃªter et supprimer les volumes (âš ï¸ perte de donnÃ©es)
docker-compose down -v

# ArrÃªter un service spÃ©cifique
docker-compose stop kafka
```

### Statut

```bash
# Voir le statut de tous les services
./status.sh

# Voir les conteneurs en cours d'exÃ©cution
docker-compose ps

# Voir l'utilisation des ressources
docker stats
```

## ðŸ“‹ Logs et DÃ©bogage

### Voir les Logs

```bash
# Logs de tous les services (temps rÃ©el)
docker-compose logs -f

# Logs d'un service spÃ©cifique
docker-compose logs -f kafka
docker-compose logs -f spark-master
docker-compose logs -f airflow-webserver

# Logs des 100 derniÃ¨res lignes
docker-compose logs --tail=100 kafka

# Logs depuis une date
docker-compose logs --since 2024-01-01T00:00:00
```

### Entrer dans un Conteneur

```bash
# Kafka
docker exec -it bigdata_kafka bash

# Spark Master
docker exec -it bigdata_spark_master bash

# Airflow
docker exec -it bigdata_airflow_webserver bash

# Jupyter
docker exec -it bigdata_jupyter bash

# PostgreSQL
docker exec -it bigdata_postgres psql -U airflow
```

## ðŸ”§ Kafka

### Topics

```bash
# CrÃ©er les topics par dÃ©faut
./scripts/setup/create_kafka_topics.sh

# Lister les topics
docker exec bigdata_kafka kafka-topics \
  --list \
  --bootstrap-server localhost:9092

# CrÃ©er un topic
docker exec bigdata_kafka kafka-topics \
  --create \
  --bootstrap-server localhost:9092 \
  --topic mon-topic \
  --partitions 3 \
  --replication-factor 1

# DÃ©crire un topic
docker exec bigdata_kafka kafka-topics \
  --describe \
  --bootstrap-server localhost:9092 \
  --topic mon-topic

# Supprimer un topic
docker exec bigdata_kafka kafka-topics \
  --delete \
  --bootstrap-server localhost:9092 \
  --topic mon-topic
```

### Producteur / Consommateur

```bash
# Producteur en ligne de commande
docker exec -it bigdata_kafka kafka-console-producer \
  --bootstrap-server localhost:9092 \
  --topic mon-topic

# Consommateur en ligne de commande (depuis le dÃ©but)
docker exec -it bigdata_kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic mon-topic \
  --from-beginning

# Consommateur avec clÃ© et valeur
docker exec -it bigdata_kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic mon-topic \
  --property print.key=true \
  --property key.separator=: \
  --from-beginning

# Groupes de consommateurs
docker exec bigdata_kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --list

# DÃ©tails d'un groupe
docker exec bigdata_kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --group mon-groupe \
  --describe
```

## âš¡ Spark

### Soumettre un Job

```bash
# Job PySpark local
docker exec bigdata_spark_master spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark-apps/mon_job.py

# Job avec arguments
docker exec bigdata_spark_master spark-submit \
  --master spark://spark-master:7077 \
  --conf spark.executor.memory=1g \
  --conf spark.executor.cores=1 \
  /opt/spark-apps/mon_job.py arg1 arg2

# Job avec dÃ©pendances
docker exec bigdata_spark_master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  /opt/spark-apps/mon_job_kafka.py
```

### PySpark Shell

```bash
# Lancer PySpark en interactif
docker exec -it bigdata_spark_master pyspark \
  --master spark://spark-master:7077

# Avec Kafka
docker exec -it bigdata_spark_master pyspark \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
```

### Monitoring

```bash
# Ouvrir Spark Master UI
open http://localhost:8082

# Ouvrir Worker 1 UI
open http://localhost:8083

# Ouvrir Worker 2 UI
open http://localhost:8084

# Voir les jobs en cours (dans Jupyter)
open http://localhost:4040
```

## ðŸŒŠ Airflow

### DAGs

```bash
# Lister les DAGs
docker exec bigdata_airflow_scheduler airflow dags list

# DÃ©clencher un DAG
docker exec bigdata_airflow_scheduler airflow dags trigger mon_dag

# Tester une tÃ¢che
docker exec bigdata_airflow_scheduler airflow tasks test mon_dag ma_tache 2024-01-01

# Voir l'Ã©tat d'un DAG
docker exec bigdata_airflow_scheduler airflow dags state mon_dag 2024-01-01
```

### Base de DonnÃ©es

```bash
# Initialiser la BD (dÃ©jÃ  fait au dÃ©marrage)
docker exec bigdata_airflow_webserver airflow db init

# CrÃ©er un utilisateur admin
docker exec bigdata_airflow_webserver airflow users create \
  --username admin \
  --firstname John \
  --lastname Doe \
  --role Admin \
  --email admin@example.com \
  --password admin

# Lister les utilisateurs
docker exec bigdata_airflow_webserver airflow users list
```

### Variables et Connections

```bash
# DÃ©finir une variable
docker exec bigdata_airflow_webserver airflow variables set ma_variable "valeur"

# Lister les variables
docker exec bigdata_airflow_webserver airflow variables list

# CrÃ©er une connexion
docker exec bigdata_airflow_webserver airflow connections add mon_conn \
  --conn-type postgres \
  --conn-host postgres \
  --conn-login airflow \
  --conn-password airflow \
  --conn-port 5432
```

## ðŸ““ Jupyter

### Gestion

```bash
# Voir le token
docker exec bigdata_jupyter jupyter server list

# Installer un package Python
docker exec bigdata_jupyter pip install nom-du-package

# Lister les packages installÃ©s
docker exec bigdata_jupyter pip list
```

## ðŸ’¾ PostgreSQL (Airflow)

```bash
# Se connecter Ã  PostgreSQL
docker exec -it bigdata_postgres psql -U airflow -d airflow

# Depuis psql:
\dt              # Lister les tables
\d+ nom_table    # DÃ©crire une table
\q               # Quitter

# Backup de la base
docker exec bigdata_postgres pg_dump -U airflow airflow > backup.sql

# Restore
docker exec -i bigdata_postgres psql -U airflow airflow < backup.sql
```

## â˜ï¸ Google Cloud Platform

### Configuration

```bash
# Tester la connexion GCP
python3 scripts/gcp/test_connection.py

# Installer les dÃ©pendances GCP
pip install google-cloud-storage google-cloud-bigquery
```

### GCS (Cloud Storage)

```bash
# Lister les buckets
gsutil ls

# Lister les fichiers d'un bucket
gsutil ls gs://mon-bucket/

# Uploader un fichier
gsutil cp fichier.txt gs://mon-bucket/

# TÃ©lÃ©charger un fichier
gsutil cp gs://mon-bucket/fichier.txt ./

# Copier un dossier
gsutil -m cp -r dossier/ gs://mon-bucket/

# Supprimer un fichier
gsutil rm gs://mon-bucket/fichier.txt

# Synchroniser un dossier
gsutil -m rsync -r dossier_local/ gs://mon-bucket/dossier_distant/
```

### BigQuery

```bash
# Lister les datasets
bq ls

# Lister les tables d'un dataset
bq ls orangescrum_dw

# DÃ©crire une table
bq show orangescrum_dw.ma_table

# ExÃ©cuter une requÃªte
bq query --use_legacy_sql=false \
  'SELECT * FROM `orangescrum_dw.ma_table` LIMIT 10'

# Charger des donnÃ©es depuis GCS
bq load \
  --source_format=PARQUET \
  orangescrum_dw.ma_table \
  gs://mon-bucket/data/*.parquet

# Exporter vers GCS
bq extract \
  --destination_format=PARQUET \
  orangescrum_dw.ma_table \
  gs://mon-bucket/export/*.parquet
```

## ðŸ§ª Tests et VÃ©rification

### PrÃ©requis

```bash
# VÃ©rifier les prÃ©requis systÃ¨me
./scripts/setup/check_prerequisites.sh
```

### Tests de ConnectivitÃ©

```bash
# Test Kafka
docker exec bigdata_kafka kafka-broker-api-versions \
  --bootstrap-server localhost:9092

# Test Spark
curl http://localhost:8082

# Test Airflow
curl http://localhost:8085/health

# Test Jupyter
curl http://localhost:8888

# Test GCP
python3 scripts/gcp/test_connection.py
```

### Healthchecks

```bash
# Voir les healthchecks de tous les services
docker ps --format "table {{.Names}}\t{{.Status}}"

# Healthcheck d'un service spÃ©cifique
docker inspect --format='{{.State.Health.Status}}' bigdata_kafka
```

## ðŸ§¹ Nettoyage

### Nettoyage LÃ©ger

```bash
# Supprimer les logs Airflow
rm -rf ./airflow/logs/*

# Supprimer les donnÃ©es de test
rm -rf ./data/raw/* ./data/processed/*
```

### Nettoyage Complet

```bash
# Nettoyer tout (âš ï¸ perte de donnÃ©es)
./clean.sh

# Ou manuellement
docker-compose down -v
docker system prune -a
```

## ðŸ“Š Monitoring

### Ressources Docker

```bash
# Utilisation en temps rÃ©el
docker stats

# Utilisation des volumes
docker system df -v

# Nettoyer les ressources inutilisÃ©es
docker system prune
```

### Ports UtilisÃ©s

```bash
# VÃ©rifier qu'un port est libre
lsof -i :8080

# Lister tous les ports utilisÃ©s par Docker
docker-compose ps
```

## ðŸ”„ RedÃ©marrage

### RedÃ©marrer un Service

```bash
# RedÃ©marrer un service
docker-compose restart kafka

# RedÃ©marrer tous les services
docker-compose restart

# RedÃ©marrer avec reconstruction
docker-compose up -d --build
```

### Forcer une Reconstruction

```bash
# Reconstruire et redÃ©marrer
docker-compose down
docker-compose up -d --build

# Reconstruire sans cache
docker-compose build --no-cache
docker-compose up -d
```

## ðŸ’¡ Astuces Utiles

### Alias Ã  Ajouter dans ~/.zshrc ou ~/.bashrc

```bash
# Ajouter ces alias pour gagner du temps
alias dc='docker-compose'
alias dcup='docker-compose up -d'
alias dcdown='docker-compose down'
alias dcps='docker-compose ps'
alias dclogs='docker-compose logs -f'

# Kafka
alias kafka-topics='docker exec bigdata_kafka kafka-topics --bootstrap-server localhost:9092'
alias kafka-producer='docker exec -it bigdata_kafka kafka-console-producer --bootstrap-server localhost:9092'
alias kafka-consumer='docker exec -it bigdata_kafka kafka-console-consumer --bootstrap-server localhost:9092'

# Spark
alias spark-submit='docker exec bigdata_spark_master spark-submit --master spark://spark-master:7077'
alias pyspark='docker exec -it bigdata_spark_master pyspark --master spark://spark-master:7077'

# Airflow
alias airflow='docker exec bigdata_airflow_scheduler airflow'
```

### Surveiller les Logs en Temps RÃ©el

```bash
# Terminal 1: Kafka
docker-compose logs -f kafka

# Terminal 2: Spark
docker-compose logs -f spark-master spark-worker-1

# Terminal 3: Airflow
docker-compose logs -f airflow-webserver airflow-scheduler
```

---

**Bon dÃ©veloppement ! ðŸš€**

Pour plus d'informations, consultez :
- **README.md** : Vue d'ensemble
- **QUICKSTART.md** : DÃ©marrage rapide
- **docs/architecture.md** : Architecture dÃ©taillÃ©e
- **docs/setup_gcp.md** : Configuration GCP

